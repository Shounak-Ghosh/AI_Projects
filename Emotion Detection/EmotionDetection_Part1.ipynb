{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "22shounakg_EmotionDetection_Section1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTvlQpRbknCZ",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7b33c17d-1aeb-4bb8-ea6c-ccb55e973d2c"
      },
      "source": [
        "#@title Run this to prepare our environment\n",
        "\n",
        "\n",
        "# Imports the required libraries\n",
        "import cv2\n",
        "import dlib\n",
        "import math\n",
        "import gdown\n",
        "import unittest\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "\n",
        "from scipy.spatial import distance\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "###Getting the Dlib Shape predictor!\n",
        "\n",
        "dlibshape_url = 'https://drive.google.com/uc?id=17D3D89Gke6i5nKOvmsbPslrGg5rVgOwg'\n",
        "dlibshape_path ='./shape_predictor_68_face_landmarks.dat'\n",
        "gdown.download(dlibshape_url, dlibshape_path, True)\n",
        "\n",
        "print (\"Done\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdMUXYoMKQnt",
        "colab_type": "text"
      },
      "source": [
        "# Emotion Detection\n",
        "\n",
        "Have you ever been in a situation where you signed up for this online course. However, after attending couple of sessions, you dropped out of the course.\n",
        "According to facts, more than 70 % people enrolled in online courses tend to dropout. Why is it so? Well, there could be multiple reasons to this such as the unadaptable teaching style, quality and the difficulty of the teaching content etc.\n",
        "\n",
        "However, if feedback system based on emotions which could predict the behavior of the student as to if they were delighted, frustrated or confused was incorporated wouldn't it make learning better?\n",
        "\n",
        "Well ofcourse it would!!! Imagine a scenario were confusion was seen as prominent emotion within the class. What it could possibly imply is either they didn't understand the content or they couldnt adhere to tutor's teaching style. This kind of feedback can be taken into account so that the upcoming sessions could be suited better to the needs of the students.\n",
        "\n",
        "Emotion Detection also has a wide variety of applications. Smart cars with facial emotion detection technology can help understand if the driver is feeling drowsy and send driver personalized alerts to stop for coffee break or change the music etc.\n",
        "\n",
        "Companies are also using emotion detection during the Video Game Testing phase. It helps them understand which emotions are experienced at what points in the game.Taking written feedback from the user who has experienced the game can be inefficient. This is because it can often be difficult to put an experience into words. \n",
        "\n",
        "Facial Emotion detection is a practical means of going beyond the spoken or written feedback and appreciating what the user is experiencing. Such kind of feedback is more reliable than other forms of feedback.\n",
        "\n",
        "\n",
        "![Demo](https://drive.google.com/uc?export=view&id=1Fad8inQU07xPMcAkeGSY7N1CmEMoWivE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltAe5xuBUl6M",
        "colab_type": "text"
      },
      "source": [
        "You and your group are going to build an AI tool that can help predict the emotions based on the Facial Expressions. \n",
        "\n",
        "You will classify the facial expressions into one of the following core emotions: **Anger, Happy, Sad, Surprise, Fear**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfScS6ztabOs",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 1 (5 Minutes) | Discussion: How should we set up this problem\n",
        "\n",
        "- What \"features\" might be useful in recognizing emotion? \n",
        "- What steps would you take to identify and use these features for emotion recognition? \n",
        "- How would you set up this problem using the tools we've learned in this course? \n",
        "- What are some applications for this machine learning approach to emotion recognition?\n",
        "- What might be missing from this approach?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_u8_a0JelHpd",
        "colab_type": "text"
      },
      "source": [
        "#Milestone 1: Understanding Face Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6i534TRm98_",
        "colab_type": "text"
      },
      "source": [
        "**What is Face Detection?**\n",
        "\n",
        "Face detection is a computer vision technology that helps to locate human faces in images. This technique is a specific use case of object detection technology that deals with detecting instances of semantic objects of a certain class (such as humans, buildings or cars) in images and videos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQzflPFunFSr",
        "colab_type": "text"
      },
      "source": [
        " \n",
        "\n",
        "![](https://cdn.xl.thumbs.canstockphoto.com/happy-woman-with-umbrella-walking-in-autumn-park-season-weather-and-people-concept-beautiful-stock-photo_csp40883320.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RU3qbfPbnFPR",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 2 (Discussion) | 5 minutes \n",
        "\n",
        "- What emotion is depicted in the image above? \n",
        "- Which part of the image helped you predict the emotion?\n",
        "- What steps did you take to recognize this emotion?\n",
        "\n",
        "You can visit [this site](https://imotions.com/blog/facial-action-coding-system) to look at how human facial expressions have been segmented into groups. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_rNLO05tEWA",
        "colab_type": "text"
      },
      "source": [
        "##Face Detection Demonstration\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iB26IT8utRwi",
        "colab_type": "text"
      },
      "source": [
        "Face Detection is an important step in the emotion classification pipeline. It helps us eliminate parts of the image which have no relevance in detecting the emotion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KR5meZxv-nq",
        "colab_type": "text"
      },
      "source": [
        "Face detection algorithms are used to predict the bounding box co-ordinates of the face\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1ZE-3eN2sarQ0h9kusTO5hosWLi2S4mlw)\n",
        "\n",
        "Dlib is a popular Python library complied in C++. For this project we will use\n",
        "Dlib's pre-trained Face detection model to extract the bounding box co-ordinates of the face\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKcl354K4cAF",
        "colab_type": "text"
      },
      "source": [
        "###Load Pretrained Dlib model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-pHGj2OkRWJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load's dlib's pretrained face detector model\n",
        "frontalface_detector = dlib.get_frontal_face_detector()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQnR1UJVmW2r",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Run this cell to define a helper Function for Face Detection\n",
        "\n",
        "'''\n",
        "  Converts dlib rectangular object to bounding box co-ordinates\n",
        "'''\n",
        "def rect_to_bb(rect):\n",
        "    # take a bounding predicted by dlib and convert it\n",
        "    # to the format (x, y, w, h) as we would normally do\n",
        "    # with OpenCV\n",
        "    x = rect.left()\n",
        "    y = rect.top()\n",
        "    w = rect.right() - x\n",
        "    h = rect.bottom() - y\n",
        "    return (x, y, w, h)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vo7m32hHnE7u",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Run this cell to define a helper Function for Face Detection with a given image\n",
        "\n",
        "\"\"\"\n",
        "Detects the face in the given image\n",
        "\"\"\"\n",
        "def detect_face(image_url):\n",
        "  \"\"\"\n",
        "  :type image_url: str\n",
        "  :rtype: None\n",
        "  \n",
        "  \"\"\"\n",
        "  try:\n",
        "    \n",
        "    #Decodes image address to cv2 object\n",
        "    url_response = urllib.request.urlopen(image_url)\n",
        "    img_array = np.array(bytearray(url_response.read()), dtype=np.uint8)\n",
        "    image = cv2.imdecode(img_array, -1)\n",
        "    \n",
        "  except Exception as e:\n",
        "    return \"Please check the URL and try again!\"\n",
        "    \n",
        "  #Detect faces using dlib model\n",
        "  rects = frontalface_detector(image, 1)\n",
        "  \n",
        "  if len(rects) < 1:\n",
        "    return \"No Face Detected\"\n",
        "  \n",
        "  # Loop over the face detections\n",
        "  for (i, rect) in enumerate(rects):\n",
        "    # Converts dlib rectangular object to bounding box co-ordinates\n",
        "    (x, y, w, h) = rect_to_bb(rect)\n",
        "    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "  plt.imshow(image, interpolation='nearest')\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "  "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llTv-nv44YX4",
        "colab_type": "text"
      },
      "source": [
        "### Face Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Erulyghe4T_a",
        "colab_type": "text"
      },
      "source": [
        "###Try it Out!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1g5S3o_euWsQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "outputId": "73120a9f-0309-4145-b9c8-07e596ebb011"
      },
      "source": [
        "# https://www.clickinmoms.com/blog/wp-content/uploads/2014/10/black-and-white-portrait-of-man-with-his-eyes-closed-by-Brian-Powers.jpg\n",
        "# https://i.pinimg.com/736x/a8/59/05/a85905aad4b379aafd63bbbd3144025d--freya-mavor-beautiful-people.jpg\n",
        "# https://i.pinimg.com/236x/27/28/0e/27280ee28567c1e20c119f74981ee5c4--black-freckles-freckles-makeup.jpg\n",
        "\n",
        "# Give the path of the image for face detection\n",
        "detect_face(input('Enter the URL of the image: '));"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-64c4c8bf3a6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Give the path of the image for face detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdetect_face\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Enter the URL of the image: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cD34pERNy6my",
        "colab_type": "text"
      },
      "source": [
        "#Milestone 2: Understanding Facial Landmarks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uaTewLrzINj",
        "colab_type": "text"
      },
      "source": [
        "**What are Facial Landmarks?**\n",
        "\n",
        "\n",
        "Facial landmarks are a set of key points on human face images/Facial Landmarks represent the points of interest within the face. The points are defined by their (x,y) coordinates on the image. These points are used to locate and represent salient regions of the face, such as eyes, eyebrows, nose, mouth and jawline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4V4NJXsN0jXS",
        "colab_type": "text"
      },
      "source": [
        "##Facial Landmark Demonstration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "520Q_tTT0Zbh",
        "colab_type": "text"
      },
      "source": [
        "Facial Landmark estimation is an important feature extraction steps in solving variety of applications such as face recognition, facial expression recognition, face swapping, face filters and much more. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVkMvanE2tsk",
        "colab_type": "text"
      },
      "source": [
        "The number of Facial key points on the face can be variable depending on the pre-trained facial landmark model being used.\n",
        "\n",
        "\n",
        "\n",
        "![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ3TMlMcOORwi88JUPO3xvHbjl8yBGDZnMMNhfpY5pS4Mvq_n7w)\n",
        "\n",
        "For this project, we will be using Dlib's pretrained Facial Landmark Detection Model which help us detect 68 2-Dimensional points on the human face\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZteCP8pz7KL",
        "colab_type": "text"
      },
      "source": [
        "## Facial Landmark Estimation using DLib\n",
        "\n",
        "In this section, we are going to look at the code to extract and plot the 68 Facial Landmarks for the given image.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CB9fqreHQJtk",
        "colab_type": "text"
      },
      "source": [
        "###Load Pre-trained DLib models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhVSMc7I1sX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load's dlib's pretrained face detector model\n",
        "frontalface_detector = dlib.get_frontal_face_detector()\n",
        "#Load the 68 face Landmark file\n",
        "landmark_predictor = dlib.shape_predictor('./shape_predictor_68_face_landmarks.dat')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAI9BJyoPoyl",
        "colab_type": "text"
      },
      "source": [
        "### Extracting Facial Landmarks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdV7TDoz14QL",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Run this cell to define a helper function for Face Detection from a url\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Returns facial landmarks for the given input image path\n",
        "\"\"\"\n",
        "def get_landmarks(image_url):\n",
        "  \"\"\"\n",
        "  :type image_url : str\n",
        "  :rtype image : cv2 object\n",
        "  :rtype landmarks : list of tuples where each tuple represents \n",
        "                     the x and y co-ordinates of facial keypoints\n",
        "  \"\"\"\n",
        "  \n",
        "  try:\n",
        "    \n",
        "    #Decodes image address to cv2 object\n",
        "    url_response = urllib.request.urlopen(image_url)\n",
        "    img_array = np.array(bytearray(url_response.read()), dtype=np.uint8)\n",
        "    image = cv2.imdecode(img_array, -1)\n",
        "    \n",
        "  except Exception as e:\n",
        "    print (\"Please check the URL and try again!\")\n",
        "    return None,None\n",
        "  \n",
        "  #Detect the Faces within the image\n",
        "  faces = frontalface_detector(image, 1)\n",
        "  if len(faces):\n",
        "    landmarks = [(p.x, p.y) for p in landmark_predictor(image, faces[0]).parts()]\n",
        "  else:\n",
        "    return None,None\n",
        "  \n",
        "  return image,landmarks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InE6rfBXPkEU",
        "colab_type": "text"
      },
      "source": [
        "###Visualizing Facial Landmarks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4npmNu0zgKz",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Run this cell to define a helper function to visualize landmarks\n",
        "\n",
        "\"\"\"\n",
        "Display image with its Facial Landmarks\n",
        "\"\"\"\n",
        "def image_landmarks(image,face_landmarks):\n",
        "  \"\"\"\n",
        "  :type image_path : str\n",
        "  :type face_landmarks : list of tuples where each tuple represents \n",
        "                     the x and y co-ordinates of facial keypoints\n",
        "  :rtype : None\n",
        "  \"\"\"\n",
        "  radius = -1\n",
        "  circle_thickness = 5\n",
        "  image_copy = image.copy()\n",
        "  for (x, y) in face_landmarks:\n",
        "    cv2.circle(image_copy, (x, y), circle_thickness, (255,0,0), radius)\n",
        "    \n",
        "  plt.imshow(image_copy, interpolation='nearest')\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Vl6hSG4P6RJ",
        "colab_type": "text"
      },
      "source": [
        "###Try it Out!!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLeqLY0Xzk1D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Extract the Facial Landmark co-ordinates\n",
        "image,landmarks= get_landmarks(input(\"Enter the URL of the image: \")) #url\n",
        "\n",
        "#Plot the Facial Landmarks on the face\n",
        "if landmarks:\n",
        "  image_landmarks(image,landmarks)\n",
        "else:\n",
        "  print (\"No Landmarks Detected\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFKqtolfgouD",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Run (and eventually edit) this cell to visualize the features we've extracted\n",
        "\n",
        "def show_indices(landmarks, i_index): \n",
        "  \n",
        "  plt.scatter(x=[landmarks[i][0] for i in range(len(landmarks)//2, len(landmarks))], \n",
        "              y=[-landmarks[i][1] for i in range(len(landmarks)//2, len(landmarks))], s=50, alpha=.5, color='blue', label='second half of indices') \n",
        "\n",
        "  plt.scatter(x=[landmarks[i][0] for i in range(len(landmarks)//2)], \n",
        "              y=[-landmarks[i][1] for i in range(len(landmarks)//2)], color='red', alpha=.5, label='first half of indices')\n",
        "\n",
        "  # what should X and Y be to visualize the feature at i_index? \n",
        "  #plt.scatter(x=X, y=-Y, \n",
        "  #            color='purple', s=100, marker='x', label='feature at index %d'%i_index)\n",
        "\n",
        "  plt.axis('off');\n",
        "  plt.legend(bbox_to_anchor=[1,1]);\n",
        "  plt.title('Visualizing the features we\\'ve extracted from this image',y =1.2); "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FX7o0LZgp6e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_index = 30\n",
        "show_indices(landmarks, show_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nanct0VrFr5S",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 2B (Discussion) | 10 Minutes | Within a student group\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUjwuDg6JjRq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Which Facial Landmark points correspond to which part of the face? { display-mode: \"form\" }\n",
        "LeftEye= \"36-41\" #@param[\"0-16\", \"17-26\", \"27-35\",\"36-41\",\"42-47\",\"48-67\",\"Fill Me In\"] \n",
        "RightEye = \"42-47\"#@param[\"0-16\", \"17-26\", \"27-35\",\"36-41\",\"42-47\",\"48-67\",\"Fill Me In\"]  \n",
        "Eyebrows = \"17-26\"#@param[\"0-16\", \"17-26\", \"27-35\",\"36-41\",\"42-47\",\"48-67\",\"Fill Me In\"] \n",
        "Nose = \"27-35\"#@param[\"0-16\", \"17-26\", \"27-35\",\"36-41\",\"42-47\",\"48-67\",\"Fill Me In\"] \n",
        "Mouth = \"48-67\"#@param[\"0-16\", \"17-26\", \"27-35\",\"36-41\",\"42-47\",\"48-67\",\"Fill Me In\"] \n",
        "Jawline = \"0-16\"#@param[\"0-16\", \"17-26\", \"27-35\",\"36-41\",\"42-47\",\"48-67\",\"Fill Me In\"] \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if LeftEye == \"36-41\": \n",
        "  print(\"The Left eye can be accessed through points %s\"%LeftEye) \n",
        "else: \n",
        "  print('Not quite %s'%LeftEye)\n",
        "  \n",
        "if RightEye == \"42-47\": \n",
        "  print(\"The Right eye can be accessed through points %s\"%RightEye) \n",
        "else: \n",
        "  print('Not quite %s'%RightEye)\n",
        "  \n",
        "if Eyebrows == \"17-26\": \n",
        "  print(\"The Eyebrows can be accessed through points %s\"%Eyebrows) \n",
        "else: \n",
        "  print('Not quite %s'%Eyebrows)\n",
        "  \n",
        "if Nose == \"27-35\": \n",
        "  print(\"The Nose can be accessed through points %s\"%Nose) \n",
        "else: \n",
        "  print('Not quite %s'%Nose)\n",
        "\n",
        "if Mouth == \"48-67\": \n",
        "  print(\"The Mouth can be accessed through points %s\"%Mouth) \n",
        "else: \n",
        "  print('Not quite %s'%Mouth)\n",
        "  \n",
        "if Jawline == \"0-16\": \n",
        "  print(\"The Jawline can be accessed through points %s\"%Jawline) \n",
        "else: \n",
        "  print('Not quite %s'%Jawline)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZBtFtUkN4RF",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 2C (Coding) \n",
        "\n",
        "In this section, you will modify the inputs to `image_landmarks`  function defined in previous section to detect and display different parts of the face individually using Facial landmarks.\n",
        "\n",
        "Write code to detect eyes, nose, mouth, jawline and eyebrows using Facial Landmarks.\n",
        "\n",
        "Hint: To detect the eyes, you need to plot Facial Landmark points from 36-47\n",
        "\n",
        "Note: Make sure you have valid facial landmark output after running the previous block (Try it Out Section!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VY6XBQWwAJ1n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i_landmark = landmarks[0:10]\n",
        "\n",
        "print(i_landmark, type(i_landmark))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHTtd0BjBtx_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(i_landmark[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RICf9HVBsqL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(landmarks[10][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYWwHTUYCGKn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(landmarks) * len(landmarks[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLmJQL72AtDY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(landmarks), len(landmarks[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-6LNqdaBIXU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "landmarks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9L6pXsoAv4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.array([0, 1,2,3])[0:3+1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qUXLt9H_L9x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i_landmark in landmarks: \n",
        "  \n",
        "  plt.scatter(x=i_landmark[0], y=-i_landmark[1], color='black')\n",
        "\n",
        "  plt.annotate(str(i_landmark), xy=(i_landmark[0], -i_landmark[1]), size=8, alpha=.5)\n",
        "plt.axis('off') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjApJ-hbKxzl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "simple = ['eyes', 'nose']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXDGyfD6Kzjr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "simple"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yvyj7E4AKB5l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "simple['eyes']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kX7f7xrq_e9j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "landmark_indices = {'eyes':(36,47),\n",
        "                    \"nose\":(27,35),\n",
        "                    \"mouth\":(48,67),\n",
        "                    \"jawline\":(0,17),\n",
        "                    \"eyebrow\":(18,27)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3xBRAOJM6oz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i_array = np.array([[1,2,3],[1,2,3]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZhRPLKgM-Mf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i_array.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efU8mDYuLurb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "landmark_indices.keys() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFKDk-OQJzfD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i_landmark in landmark_indices.keys():\n",
        "\n",
        "  i_indices = landmark_indices[i_landmark]\n",
        "  selected_landmarks = landmarks[i_indices[0]:i_indices[1]]\n",
        "  image_landmarks(image,selected_landmarks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSMs3_cNLdyR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(landmark_indices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UB7VVqGyNdOn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "landmark_indices.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJ46v5CZJvxO",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDvFceG3N17i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Display images with individual detection of face parts \n",
        "\n",
        "# For example, for eye detection\n",
        "\n",
        "eye_points = np.array([36,47])\n",
        "selected_landmarks = landmarks[eye_points[0]:eye_points[1]+1]\n",
        "print(selected_landmarks)\n",
        "image_landmarks(image,selected_landmarks)\n",
        "\n",
        "### YOUR CODE HERE\n",
        "FACIAL_LANDMARKS_IDXS = {\"EYES\":(36,47),\n",
        "                         \"NOSE\":(27,35),\n",
        "                         \"MOUTH\":(48,67),\n",
        "                        \"JAWLINE\":(0,17),\n",
        "                        \"EYEBROWS\":(18,27)}\n",
        "\n",
        "for key,value in FACIAL_LANDMARKS_IDXS.items():\n",
        "  print (key,\"DETECTION\")\n",
        "  selected_landmarks = landmarks[value[0]:value[1]+1]\n",
        "  image_landmarks(image, selected_landmarks)\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A355Uv7wDMDl",
        "colab_type": "text"
      },
      "source": [
        "#Milestone 3: Understanding Euclidean Distances\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hbf8TZPw-A7r",
        "colab_type": "text"
      },
      "source": [
        "Euclidean distance between between points p and q is is equal to the length of the line segment connecting them. When data is dense or continuous, this is the best proximity measure. \n",
        "\n",
        "In this section, we will explore how euclidean distance between pairs of Facial Landmarks can help solve simple use cases related to faces"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10B9RCvzC6y7",
        "colab_type": "text"
      },
      "source": [
        "##Exercise 3A (Discussion) | 5 Minutes | Within a student group"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NockIqDUmCEM",
        "colab_type": "text"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1UTtJOQRh6ebj86SIPr-WfdRgDS2nb3m_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtogEi-XkmVP",
        "colab_type": "text"
      },
      "source": [
        "###What is the difference between two images? Can you use facial landmarks  to distinguish between the two images?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgWCy3ho-s2U",
        "colab_type": "text"
      },
      "source": [
        "Take a look at the images with facial landmarks superimposed over them!\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=16O3KupWH090-9avnsah8v1LcM80okgmX)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XQvxhwvSnYR",
        "colab_type": "text"
      },
      "source": [
        "Does the distance between certain facial landmarks help us distinguish between two images?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4awukiot2tu",
        "colab_type": "text"
      },
      "source": [
        "##Exercise 3B (Coding) | Are the eyes open or close ?\n",
        "\n",
        "In last block, we figured out which set of landmarks help us distinguish between the two images. In this section, you will write code to distinguish between closed eyes and open eyes using facial Landmarks\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmSPfiVZ-_7a",
        "colab_type": "text"
      },
      "source": [
        "###Euclidean Distance\n",
        "\n",
        "Write a function to compute the euclidean distance between two points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_IsP9gUpcun",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Computes the euclidean distance between 2 points in 2D space\n",
        "#inexing #math.sqrt\n",
        "\"\"\"\n",
        "def euclidean_distance(p1,p2):\n",
        "  \"\"\"\n",
        "  type p1, p2 : tuple--> (x,y)\n",
        "  rtype distance: float\n",
        "  \"\"\"\n",
        "  distance =  math.sqrt((p2[0]-p1[0])**2 + (p2[1]-p1[1])**2)\n",
        "  return distance\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xw2ZuICTir0Q",
        "colab_type": "text"
      },
      "source": [
        "### Are there other distance metrics we can use here? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQZnD_48_Dl8",
        "colab_type": "text"
      },
      "source": [
        "###Classify images based on eyes\n",
        "\n",
        "Write code to find out which image corresponds to closed eyes and which image corresponds to open eyes using the concept of euclidean distance\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7enTs1C7Jj8",
        "colab_type": "text"
      },
      "source": [
        "###Psuedo-Algorithm\n",
        "\n",
        "1. Identity the Facial Landmarks of Interest\n",
        "2. Compute the distances between the points of interest\n",
        "3. Compare the distances of both the images\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7yQiH3Q05yi",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\"\"\"\n",
        "Distinguishes between two images--->closed eyes v/s open eyes \n",
        "\"\"\"\n",
        "def classify_images(image1_path, image2_path, plt_flag):\n",
        "  \"\"\"\n",
        "  type image1_path,image2_path: str\n",
        "  rtype : str\n",
        "  \"\"\"\n",
        "  \n",
        "  image1,image1_landmarks = get_landmarks(image1_path)\n",
        "  image2,image2_landmarks = get_landmarks(image2_path)\n",
        "  \n",
        "  if plt_flag:\n",
        "    #Plot image1\n",
        "    plt.imshow(image1, interpolation='nearest')\n",
        "    plt.title(\"Image1\")\n",
        "    plt.show()\n",
        "\n",
        "    #Plot image2\n",
        "    plt.imshow(image2, interpolation='nearest')\n",
        "    plt.title(\"Image2\")\n",
        "    plt.show()\n",
        "    \n",
        "  \n",
        "  # Points of interest for eyes among which distance needs to be computed\n",
        "  pairs_distance = [(37,41),(38,40),(43,47),(44,48)]\n",
        "  \n",
        "  e_sum1 = 0\n",
        "  e_sum2 = 0\n",
        "  threshold_value = 10\n",
        "  for pair in pairs_distance:\n",
        "    \n",
        "    e_sum1 = e_sum1 + euclidean_distance(image1_landmarks[pair[0]],\n",
        "                                         image1_landmarks[pair[1]])\n",
        "    e_sum2 = e_sum2 + euclidean_distance(image2_landmarks[pair[0]],\n",
        "                                         image2_landmarks[pair[1]])\n",
        "  print (e_sum1,e_sum2)\n",
        "  \n",
        "  e_difference = e_sum1 - e_sum2\n",
        "  print (e_difference)\n",
        "  if int(e_difference) == 0:\n",
        "    return (\"Both images have eyes open or closed\")\n",
        "  \n",
        "  if abs(e_difference) >= threshold_value:\n",
        "     \n",
        "    if e_difference > 0:\n",
        "        return (\"Image1 : Eyes Open, Image2 : Eyes Close\")\n",
        "    else:\n",
        "        return (\"Image1 : Eyes Close, Image2 : Eyes Open\")\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csUQshl1fer7",
        "colab_type": "text"
      },
      "source": [
        "#Milestone 4: Understanding Emotion Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPUWsQkJr32l",
        "colab_type": "text"
      },
      "source": [
        "##What distinguishes one emotion from another ?\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1rOn9Xc8-WZYuuE0IkLHhIR9pIvL0JJmx)"
      ]
    }
  ]
}