{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "22shounakg_Section_3_FakeNews.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ghImXmSD00AL"
      },
      "source": [
        "# Day 3: Using Word Vectors for Fake News Classification\n",
        "\n",
        "Run the below cells to get started."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ewFL_vL20vMS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9d17de48-3c5c-48dd-9a74-b083cac08bd0"
      },
      "source": [
        "#@title Import Data { display-mode: \"form\" }\n",
        "import math\n",
        "import os\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from torchtext.vocab import GloVe\n",
        "\n",
        "import pickle\n",
        "\n",
        "import requests, io, zipfile\n",
        "# Download class resources...\n",
        "r = requests.get(\"https://www.dropbox.com/s/2pj07qip0ei09xt/inspirit_fake_news_resources.zip?dl=1\")\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "z.extractall()\n",
        "\n",
        "basepath = '.'\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "with open(os.path.join(basepath, 'train_val_data.pkl'), 'rb') as f:\n",
        "  train_data, val_data = pickle.load(f)\n",
        "  \n",
        "print('Number of train examples:', len(train_data))\n",
        "print('Number of val examples:', len(val_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of train examples: 2002\n",
            "Number of val examples: 309\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "khlmTwTFQyFY"
      },
      "source": [
        "One potential source of information for websites is their descriptions (often called meta descriptions). These are descriptions embedded into the HTML of a webpage that describe what the website is about, so that search engines and other crawlers can use it to determine the content of a website. For example, here is the description for google.com, retrieved using the BeautifulSoup Python library for parsing HTML:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "49IV8jyXSbt5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "026b4274-496d-4679-b41f-c55036e5e66a"
      },
      "source": [
        "def get_description_from_html(html):\n",
        "  soup = bs(html)\n",
        "  description_tag = soup.find('meta', attrs={'name':'og:description'}) or soup.find('meta', attrs={'property':'description'}) or soup.find('meta', attrs={'name':'description'})\n",
        "  if description_tag:\n",
        "    description = description_tag.get('content') or ''\n",
        "  else: # If there is no description, return empty string.\n",
        "    description = ''\n",
        "  return description\n",
        "\n",
        "def scrape_description(url):\n",
        "  if not url.startswith('http'):\n",
        "    url = 'http://' + url\n",
        "  response = requests.get(url, timeout=10)\n",
        "  html = response.text\n",
        "  description = get_description_from_html(html)\n",
        "  return description\n",
        "\n",
        "print('Description of Google.com:')\n",
        "print(scrape_description('google.com'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Description of Google.com:\n",
            "Search the world's information, including webpages, images, videos and more. Google has many special features to help you find exactly what you're looking for.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGKIqXbcZyDX",
        "colab_type": "text"
      },
      "source": [
        "## Instructor-Led Discussion: Scraping Website Descriptions\n",
        "\n",
        "Play around with the below demo (running the cell normally) to live scrape the descriptions of different websites using the code above. Do you notice anything interesting or unexpected? If so, share with the class. (~3 minutes)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYro0RPbY3hl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "72c0e713-d3ed-4a7e-a5a5-71361528ac3e"
      },
      "source": [
        "#@title Live Website Description Scraper { display-mode: \"both\" }\n",
        "\n",
        "url = \"youtube.com\" #@param {type:\"string\"}\n",
        "\n",
        "print('Description of %s:' % url)\n",
        "print(scrape_description(url))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Description of youtube.com:\n",
            "Enjoy the videos and music you love, upload original content, and share it all with friends, family, and the world on YouTube.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rUc1jWs1Smvx"
      },
      "source": [
        "### Bag-of-Words Model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LurXHhf3KIoa",
        "colab_type": "text"
      },
      "source": [
        "It is easy to retrieve the descriptions for the fake and real news websites in our dataset as well. What can we do with these? We can use the approach from yesterday where we extract counts from the description for particular keywords and use these as features, but this would require us to manually select features that we think are important. What if our model automatically collected all of the most important keywords and added their counts for each website description to our feature vector? Our model could then learn feature weights for these words to help us correctly classify news websites.\n",
        "\n",
        "This approach of automatically featurizing the counts of words in text is called the bag-of-words model. This name comes from the fact that the features do not store the order of the words, rather just their counts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrZLFmRIKGBa",
        "colab_type": "text"
      },
      "source": [
        "## Exercise \n",
        "\n",
        "Let's start by extracting the descriptions for the websites in our dataset. Use the helper function *get_description_from_html* defined above to extract all of the descriptions for the websites in the training data. The return value of the function should be a list of descriptions, in the same order as the sites in *train_data* (~10 minutes). Note that running the function will take a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJQY3DU1Iza9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "be1455b1-8b06-4360-96a9-832981fe3e20"
      },
      "source": [
        "def get_descriptions_from_data(data):\n",
        "  # A dictionary mapping from url to description for the websites in \n",
        "  # train_data.\n",
        "  descriptions = []\n",
        "  for site in tqdm(data):\n",
        "    ### YOUR CODE HERE ###\n",
        "    descriptions.append(get_description_from_html(site[1]))\n",
        "    \n",
        "    ### END CODE ###\n",
        "  return descriptions\n",
        "  \n",
        "\n",
        "train_descriptions = get_descriptions_from_data(train_data)\n",
        "train_urls = [url for (url, html, label) in train_data]\n",
        "\n",
        "print('\\nNYTimes Description:')\n",
        "print(train_descriptions[train_urls.index('nytimes.com')])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/2002 [00:00<?, ?it/s]\u001b[A\n",
            "  0%|          | 3/2002 [00:00<02:07, 15.62it/s]\u001b[A\n",
            "  0%|          | 5/2002 [00:00<03:13, 10.30it/s]\u001b[A\n",
            "  0%|          | 8/2002 [00:00<02:35, 12.82it/s]\u001b[A\n",
            "  0%|          | 10/2002 [00:00<02:23, 13.88it/s]\u001b[A\n",
            "  1%|          | 14/2002 [00:00<01:58, 16.80it/s]\u001b[A\n",
            "  1%|          | 18/2002 [00:00<01:38, 20.08it/s]\u001b[A\n",
            "  1%|          | 25/2002 [00:01<01:32, 21.34it/s]\u001b[A\n",
            "  1%|▏         | 29/2002 [00:01<01:25, 23.13it/s]\u001b[A\n",
            "  2%|▏         | 34/2002 [00:01<01:15, 26.03it/s]\u001b[A\n",
            "  2%|▏         | 37/2002 [00:01<01:21, 24.01it/s]\u001b[A\n",
            "  2%|▏         | 41/2002 [00:01<01:36, 20.23it/s]\u001b[A\n",
            "  2%|▏         | 44/2002 [00:02<01:36, 20.36it/s]\u001b[A\n",
            "  2%|▏         | 50/2002 [00:02<01:18, 24.98it/s]\u001b[A\n",
            "  3%|▎         | 54/2002 [00:02<01:18, 24.71it/s]\u001b[A\n",
            "  3%|▎         | 57/2002 [00:02<01:35, 20.29it/s]\u001b[A\n",
            "  3%|▎         | 60/2002 [00:02<01:30, 21.40it/s]\u001b[A\n",
            "  3%|▎         | 63/2002 [00:02<01:25, 22.77it/s]\u001b[A\n",
            "  3%|▎         | 67/2002 [00:02<01:21, 23.82it/s]\u001b[A\n",
            "  4%|▎         | 71/2002 [00:03<01:34, 20.47it/s]\u001b[A\n",
            "  4%|▎         | 75/2002 [00:03<01:35, 20.14it/s]\u001b[A\n",
            "  4%|▍         | 80/2002 [00:03<01:26, 22.15it/s]\u001b[A\n",
            "  4%|▍         | 83/2002 [00:03<01:20, 23.77it/s]\u001b[A\n",
            "  4%|▍         | 87/2002 [00:03<01:12, 26.31it/s]\u001b[A\n",
            "  4%|▍         | 90/2002 [00:03<01:18, 24.50it/s]\u001b[A\n",
            "  5%|▍         | 93/2002 [00:05<04:29,  7.08it/s]\u001b[A\n",
            "  5%|▍         | 96/2002 [00:05<03:43,  8.51it/s]\u001b[A\n",
            "  5%|▍         | 98/2002 [00:05<03:25,  9.24it/s]\u001b[A\n",
            "  5%|▌         | 102/2002 [00:05<03:09, 10.01it/s]\u001b[A\n",
            "  5%|▌         | 105/2002 [00:06<02:54, 10.85it/s]\u001b[A\n",
            "  5%|▌         | 109/2002 [00:06<02:23, 13.22it/s]\u001b[A\n",
            "  6%|▌         | 112/2002 [00:06<02:30, 12.54it/s]\u001b[A\n",
            "  6%|▌         | 115/2002 [00:06<02:13, 14.13it/s]\u001b[A\n",
            "  6%|▌         | 117/2002 [00:06<02:17, 13.74it/s]\u001b[A\n",
            "  6%|▌         | 119/2002 [00:06<02:11, 14.35it/s]\u001b[A\n",
            "  6%|▌         | 123/2002 [00:07<02:04, 15.05it/s]\u001b[A\n",
            "  6%|▌         | 125/2002 [00:07<01:58, 15.89it/s]\u001b[A\n",
            "  6%|▋         | 129/2002 [00:07<01:40, 18.72it/s]\u001b[A\n",
            "  7%|▋         | 132/2002 [00:07<02:47, 11.18it/s]\u001b[A\n",
            "  7%|▋         | 135/2002 [00:08<02:37, 11.85it/s]\u001b[A\n",
            "  7%|▋         | 142/2002 [00:08<01:59, 15.54it/s]\u001b[A\n",
            "  7%|▋         | 145/2002 [00:08<01:46, 17.47it/s]\u001b[A\n",
            "  7%|▋         | 148/2002 [00:08<02:36, 11.83it/s]\u001b[A\n",
            "  8%|▊         | 151/2002 [00:08<02:12, 13.93it/s]\u001b[A\n",
            "  8%|▊         | 155/2002 [00:09<02:08, 14.37it/s]\u001b[A\n",
            "  8%|▊         | 158/2002 [00:09<01:54, 16.13it/s]\u001b[A\n",
            "  8%|▊         | 161/2002 [00:09<01:38, 18.72it/s]\u001b[A\n",
            "  8%|▊         | 164/2002 [00:09<01:59, 15.44it/s]\u001b[A\n",
            "  8%|▊         | 168/2002 [00:09<01:41, 18.09it/s]\u001b[A\n",
            "  9%|▊         | 172/2002 [00:10<01:46, 17.25it/s]\u001b[A\n",
            "  9%|▊         | 175/2002 [00:10<01:58, 15.44it/s]\u001b[A\n",
            "  9%|▉         | 177/2002 [00:10<01:55, 15.87it/s]\u001b[A\n",
            "  9%|▉         | 181/2002 [00:10<01:56, 15.59it/s]\u001b[A\n",
            "  9%|▉         | 184/2002 [00:10<01:40, 18.02it/s]\u001b[A\n",
            "  9%|▉         | 188/2002 [00:10<01:27, 20.68it/s]\u001b[A\n",
            " 10%|▉         | 191/2002 [00:11<01:24, 21.40it/s]\u001b[A\n",
            " 10%|▉         | 194/2002 [00:11<01:51, 16.16it/s]\u001b[A\n",
            " 10%|▉         | 196/2002 [00:11<02:05, 14.36it/s]\u001b[A\n",
            " 10%|█         | 201/2002 [00:11<01:42, 17.63it/s]\u001b[A\n",
            " 10%|█         | 204/2002 [00:11<02:01, 14.78it/s]\u001b[A\n",
            " 10%|█         | 208/2002 [00:12<01:43, 17.40it/s]\u001b[A\n",
            " 11%|█         | 213/2002 [00:12<01:25, 20.82it/s]\u001b[A\n",
            " 11%|█         | 217/2002 [00:12<01:27, 20.34it/s]\u001b[A\n",
            " 11%|█         | 220/2002 [00:12<01:24, 21.16it/s]\u001b[A\n",
            " 11%|█         | 223/2002 [00:12<01:54, 15.53it/s]\u001b[A\n",
            " 11%|█▏        | 227/2002 [00:12<01:40, 17.65it/s]\u001b[A\n",
            " 11%|█▏        | 230/2002 [00:13<02:00, 14.73it/s]\u001b[A\n",
            " 12%|█▏        | 232/2002 [00:13<02:32, 11.62it/s]\u001b[A\n",
            " 12%|█▏        | 234/2002 [00:19<27:06,  1.09it/s]\u001b[A\n",
            " 12%|█▏        | 237/2002 [00:19<19:25,  1.51it/s]\u001b[A\n",
            " 12%|█▏        | 239/2002 [00:19<14:12,  2.07it/s]\u001b[A\n",
            " 12%|█▏        | 241/2002 [00:19<10:31,  2.79it/s]\u001b[A\n",
            " 12%|█▏        | 247/2002 [00:20<08:05,  3.62it/s]\u001b[A\n",
            " 12%|█▏        | 250/2002 [00:20<06:06,  4.78it/s]\u001b[A\n",
            " 13%|█▎        | 252/2002 [00:20<04:46,  6.10it/s]\u001b[A\n",
            " 13%|█▎        | 255/2002 [00:20<03:38,  7.99it/s]\u001b[A\n",
            " 13%|█▎        | 257/2002 [00:20<03:16,  8.88it/s]\u001b[A\n",
            " 13%|█▎        | 259/2002 [00:20<02:53, 10.04it/s]\u001b[A\n",
            " 13%|█▎        | 264/2002 [00:20<02:13, 13.00it/s]\u001b[A\n",
            " 13%|█▎        | 267/2002 [00:21<01:53, 15.34it/s]\u001b[A\n",
            " 13%|█▎        | 270/2002 [00:21<01:58, 14.59it/s]\u001b[A\n",
            " 14%|█▎        | 273/2002 [00:21<02:09, 13.35it/s]\u001b[A\n",
            " 14%|█▎        | 275/2002 [00:21<02:04, 13.88it/s]\u001b[A\n",
            " 14%|█▍        | 278/2002 [00:21<01:54, 14.99it/s]\u001b[A\n",
            " 14%|█▍        | 280/2002 [00:22<01:57, 14.62it/s]\u001b[A\n",
            " 14%|█▍        | 282/2002 [00:22<02:11, 13.07it/s]\u001b[A\n",
            " 14%|█▍        | 285/2002 [00:22<01:49, 15.66it/s]\u001b[A\n",
            " 14%|█▍        | 288/2002 [00:22<01:45, 16.21it/s]\u001b[A\n",
            " 15%|█▍        | 293/2002 [00:22<01:27, 19.46it/s]\u001b[A\n",
            " 15%|█▍        | 296/2002 [00:22<01:52, 15.16it/s]\u001b[A\n",
            " 15%|█▌        | 301/2002 [00:23<01:31, 18.65it/s]\u001b[A\n",
            " 15%|█▌        | 304/2002 [00:28<17:19,  1.63it/s]\u001b[A\n",
            " 15%|█▌        | 308/2002 [00:28<12:28,  2.26it/s]\u001b[A\n",
            " 15%|█▌        | 310/2002 [00:29<09:10,  3.07it/s]\u001b[A\n",
            " 16%|█▌        | 312/2002 [00:29<08:16,  3.41it/s]\u001b[A\n",
            " 16%|█▌        | 314/2002 [00:29<06:17,  4.48it/s]\u001b[A\n",
            " 16%|█▌        | 319/2002 [00:29<04:37,  6.07it/s]\u001b[A\n",
            " 16%|█▌        | 322/2002 [00:30<04:07,  6.78it/s]\u001b[A\n",
            " 16%|█▌        | 325/2002 [00:30<03:10,  8.79it/s]\u001b[A\n",
            " 16%|█▋        | 328/2002 [00:30<02:51,  9.75it/s]\u001b[A\n",
            " 17%|█▋        | 331/2002 [00:30<02:17, 12.18it/s]\u001b[A\n",
            " 17%|█▋        | 334/2002 [00:30<01:55, 14.50it/s]\u001b[A\n",
            " 17%|█▋        | 338/2002 [00:30<01:52, 14.79it/s]\u001b[A\n",
            " 17%|█▋        | 341/2002 [00:31<01:58, 13.97it/s]\u001b[A\n",
            " 17%|█▋        | 343/2002 [00:31<02:39, 10.40it/s]\u001b[A\n",
            " 17%|█▋        | 348/2002 [00:31<02:17, 12.04it/s]\u001b[A\n",
            " 17%|█▋        | 350/2002 [00:31<02:16, 12.07it/s]\u001b[A\n",
            " 18%|█▊        | 352/2002 [00:32<02:08, 12.81it/s]\u001b[A\n",
            " 18%|█▊        | 355/2002 [00:32<01:47, 15.36it/s]\u001b[A\n",
            " 18%|█▊        | 358/2002 [00:32<01:36, 16.98it/s]\u001b[A\n",
            " 18%|█▊        | 365/2002 [00:32<01:22, 19.84it/s]\u001b[A\n",
            " 18%|█▊        | 370/2002 [00:32<01:11, 22.95it/s]\u001b[A\n",
            " 19%|█▊        | 373/2002 [00:32<01:21, 20.08it/s]\u001b[A\n",
            " 19%|█▉        | 378/2002 [00:33<01:38, 16.51it/s]\u001b[A\n",
            " 19%|█▉        | 383/2002 [00:33<01:20, 20.00it/s]\u001b[A\n",
            " 19%|█▉        | 386/2002 [00:33<01:29, 18.07it/s]\u001b[A\n",
            " 19%|█▉        | 389/2002 [00:33<01:22, 19.66it/s]\u001b[A\n",
            " 20%|█▉        | 393/2002 [00:33<01:29, 18.05it/s]\u001b[A\n",
            " 20%|█▉        | 396/2002 [00:34<01:29, 17.96it/s]\u001b[A\n",
            " 20%|██        | 402/2002 [00:34<01:14, 21.62it/s]\u001b[A\n",
            " 20%|██        | 405/2002 [00:34<01:08, 23.47it/s]\u001b[A\n",
            " 20%|██        | 408/2002 [00:34<01:34, 16.89it/s]\u001b[A\n",
            " 21%|██        | 411/2002 [00:35<01:58, 13.47it/s]\u001b[A\n",
            " 21%|██        | 414/2002 [00:35<01:42, 15.50it/s]\u001b[A\n",
            " 21%|██        | 417/2002 [00:35<01:34, 16.80it/s]\u001b[A\n",
            " 21%|██        | 422/2002 [00:35<01:17, 20.36it/s]\u001b[A\n",
            " 21%|██        | 425/2002 [00:35<01:14, 21.29it/s]\u001b[A\n",
            " 21%|██▏       | 428/2002 [00:35<01:20, 19.60it/s]\u001b[A\n",
            " 22%|██▏       | 432/2002 [00:35<01:08, 22.94it/s]\u001b[A\n",
            " 22%|██▏       | 435/2002 [00:35<01:17, 20.10it/s]\u001b[A\n",
            " 22%|██▏       | 438/2002 [00:36<01:16, 20.42it/s]\u001b[A\n",
            " 22%|██▏       | 441/2002 [00:36<01:27, 17.91it/s]\u001b[A\n",
            " 22%|██▏       | 447/2002 [00:36<01:12, 21.58it/s]\u001b[A\n",
            " 23%|██▎       | 451/2002 [00:36<01:07, 22.88it/s]\u001b[A\n",
            " 23%|██▎       | 457/2002 [00:36<00:56, 27.22it/s]\u001b[A\n",
            " 23%|██▎       | 461/2002 [00:37<01:14, 20.71it/s]\u001b[A\n",
            " 23%|██▎       | 464/2002 [00:37<01:07, 22.74it/s]\u001b[A\n",
            " 23%|██▎       | 468/2002 [00:37<01:00, 25.45it/s]\u001b[A\n",
            " 24%|██▎       | 472/2002 [00:37<01:12, 21.04it/s]\u001b[A\n",
            " 24%|██▎       | 475/2002 [00:37<01:37, 15.69it/s]\u001b[A\n",
            " 24%|██▍       | 478/2002 [00:38<01:42, 14.92it/s]\u001b[A\n",
            " 24%|██▍       | 480/2002 [00:38<01:58, 12.88it/s]\u001b[A\n",
            " 24%|██▍       | 483/2002 [00:38<01:46, 14.30it/s]\u001b[A\n",
            " 24%|██▍       | 485/2002 [00:38<01:48, 14.00it/s]\u001b[A\n",
            " 24%|██▍       | 488/2002 [00:38<01:36, 15.61it/s]\u001b[A\n",
            " 24%|██▍       | 490/2002 [00:38<01:39, 15.27it/s]\u001b[A\n",
            " 25%|██▍       | 493/2002 [00:38<01:26, 17.42it/s]\u001b[A\n",
            " 25%|██▍       | 496/2002 [00:39<01:17, 19.41it/s]\u001b[A\n",
            " 25%|██▍       | 499/2002 [00:39<01:43, 14.55it/s]\u001b[A\n",
            " 25%|██▌       | 501/2002 [00:39<01:40, 14.92it/s]\u001b[A\n",
            " 25%|██▌       | 508/2002 [00:39<01:18, 19.07it/s]\u001b[A\n",
            " 26%|██▌       | 513/2002 [00:39<01:05, 22.66it/s]\u001b[A\n",
            " 26%|██▌       | 517/2002 [00:40<01:14, 19.85it/s]\u001b[A\n",
            " 26%|██▌       | 520/2002 [00:40<01:12, 20.33it/s]\u001b[A\n",
            " 26%|██▌       | 523/2002 [00:40<01:14, 19.92it/s]\u001b[A\n",
            " 26%|██▋       | 526/2002 [00:40<01:07, 21.77it/s]\u001b[A\n",
            " 26%|██▋       | 529/2002 [00:40<01:27, 16.85it/s]\u001b[A\n",
            " 27%|██▋       | 536/2002 [00:40<01:10, 20.87it/s]\u001b[A\n",
            " 27%|██▋       | 539/2002 [00:41<01:17, 18.96it/s]\u001b[A\n",
            " 27%|██▋       | 543/2002 [00:41<01:05, 22.34it/s]\u001b[A\n",
            " 27%|██▋       | 546/2002 [00:41<01:19, 18.28it/s]\u001b[A\n",
            " 27%|██▋       | 549/2002 [00:41<01:21, 17.92it/s]\u001b[A\n",
            " 28%|██▊       | 555/2002 [00:41<01:11, 20.35it/s]\u001b[A\n",
            " 28%|██▊       | 558/2002 [00:42<01:52, 12.83it/s]\u001b[A\n",
            " 28%|██▊       | 560/2002 [00:42<02:02, 11.73it/s]\u001b[A\n",
            " 28%|██▊       | 565/2002 [00:42<01:39, 14.51it/s]\u001b[A\n",
            " 28%|██▊       | 568/2002 [00:42<01:39, 14.36it/s]\u001b[A\n",
            " 28%|██▊       | 570/2002 [00:43<02:01, 11.81it/s]\u001b[A\n",
            " 29%|██▉       | 576/2002 [00:43<01:35, 14.96it/s]\u001b[A\n",
            " 29%|██▉       | 581/2002 [00:43<01:21, 17.43it/s]\u001b[A\n",
            " 29%|██▉       | 584/2002 [00:43<01:13, 19.32it/s]\u001b[A\n",
            " 29%|██▉       | 587/2002 [00:43<01:47, 13.18it/s]\u001b[A\n",
            " 29%|██▉       | 590/2002 [00:44<01:45, 13.39it/s]\u001b[A\n",
            " 30%|██▉       | 592/2002 [00:44<01:42, 13.77it/s]\u001b[A\n",
            " 30%|██▉       | 594/2002 [00:44<01:48, 12.96it/s]\u001b[A\n",
            " 30%|██▉       | 597/2002 [00:44<01:30, 15.56it/s]\u001b[A\n",
            " 30%|██▉       | 600/2002 [00:44<01:27, 16.10it/s]\u001b[A\n",
            " 30%|███       | 602/2002 [00:44<01:23, 16.85it/s]\u001b[A\n",
            " 30%|███       | 605/2002 [00:44<01:16, 18.25it/s]\u001b[A\n",
            " 30%|███       | 608/2002 [00:45<01:29, 15.63it/s]\u001b[A\n",
            " 31%|███       | 612/2002 [00:45<01:13, 19.01it/s]\u001b[A\n",
            " 31%|███       | 615/2002 [00:45<01:05, 21.21it/s]\u001b[A\n",
            " 31%|███       | 618/2002 [00:45<01:18, 17.65it/s]\u001b[A\n",
            " 31%|███       | 621/2002 [00:45<01:30, 15.26it/s]\u001b[A\n",
            " 31%|███       | 623/2002 [00:51<20:48,  1.10it/s]\u001b[A\n",
            " 31%|███       | 625/2002 [00:51<15:14,  1.51it/s]\u001b[A\n",
            " 31%|███▏      | 627/2002 [00:52<11:50,  1.93it/s]\u001b[A\n",
            " 31%|███▏      | 629/2002 [00:52<08:58,  2.55it/s]\u001b[A\n",
            " 32%|███▏      | 631/2002 [00:52<06:40,  3.42it/s]\u001b[A\n",
            " 32%|███▏      | 634/2002 [00:52<05:16,  4.33it/s]\u001b[A\n",
            " 32%|███▏      | 636/2002 [00:53<04:48,  4.74it/s]\u001b[A\n",
            " 32%|███▏      | 641/2002 [00:53<03:33,  6.37it/s]\u001b[A\n",
            " 32%|███▏      | 643/2002 [00:53<03:05,  7.34it/s]\u001b[A\n",
            " 32%|███▏      | 648/2002 [00:53<02:19,  9.73it/s]\u001b[A\n",
            " 33%|███▎      | 651/2002 [00:53<02:12, 10.17it/s]\u001b[A\n",
            " 33%|███▎      | 653/2002 [00:53<02:05, 10.75it/s]\u001b[A\n",
            " 33%|███▎      | 655/2002 [00:54<01:55, 11.66it/s]\u001b[A\n",
            " 33%|███▎      | 658/2002 [00:54<01:34, 14.24it/s]\u001b[A\n",
            " 33%|███▎      | 661/2002 [00:54<01:39, 13.49it/s]\u001b[A\n",
            " 33%|███▎      | 665/2002 [00:54<01:22, 16.21it/s]\u001b[A\n",
            " 33%|███▎      | 668/2002 [00:54<01:15, 17.76it/s]\u001b[A\n",
            " 34%|███▎      | 671/2002 [00:54<01:15, 17.56it/s]\u001b[A\n",
            " 34%|███▎      | 674/2002 [00:55<01:30, 14.72it/s]\u001b[A\n",
            " 34%|███▍      | 676/2002 [00:55<01:33, 14.14it/s]\u001b[A\n",
            " 34%|███▍      | 679/2002 [00:55<01:22, 16.12it/s]\u001b[A\n",
            " 34%|███▍      | 681/2002 [00:55<01:18, 16.87it/s]\u001b[A\n",
            " 34%|███▍      | 684/2002 [00:55<01:10, 18.76it/s]\u001b[A\n",
            " 34%|███▍      | 687/2002 [00:55<01:17, 17.03it/s]\u001b[A\n",
            " 34%|███▍      | 689/2002 [00:56<01:20, 16.38it/s]\u001b[A\n",
            " 35%|███▍      | 691/2002 [00:56<01:24, 15.47it/s]\u001b[A\n",
            " 35%|███▍      | 693/2002 [00:56<01:42, 12.82it/s]\u001b[A\n",
            " 35%|███▍      | 696/2002 [00:56<01:40, 13.03it/s]\u001b[A\n",
            " 35%|███▍      | 698/2002 [00:56<01:39, 13.13it/s]\u001b[A\n",
            " 35%|███▌      | 701/2002 [00:56<01:27, 14.93it/s]\u001b[A\n",
            " 35%|███▌      | 703/2002 [00:57<01:23, 15.51it/s]\u001b[A\n",
            " 35%|███▌      | 707/2002 [00:57<01:24, 15.25it/s]\u001b[A\n",
            " 35%|███▌      | 710/2002 [00:57<01:16, 16.99it/s]\u001b[A\n",
            " 36%|███▌      | 713/2002 [00:57<01:20, 15.93it/s]\u001b[A\n",
            " 36%|███▌      | 717/2002 [00:57<01:08, 18.69it/s]\u001b[A\n",
            " 36%|███▌      | 723/2002 [00:57<00:58, 21.98it/s]\u001b[A\n",
            " 36%|███▋      | 726/2002 [00:58<01:08, 18.64it/s]\u001b[A\n",
            " 37%|███▋      | 731/2002 [00:58<00:58, 21.65it/s]\u001b[A\n",
            " 37%|███▋      | 734/2002 [00:58<01:07, 18.91it/s]\u001b[A\n",
            " 37%|███▋      | 737/2002 [00:58<01:03, 19.85it/s]\u001b[A\n",
            " 37%|███▋      | 740/2002 [00:58<01:11, 17.68it/s]\u001b[A\n",
            " 37%|███▋      | 744/2002 [00:59<01:06, 18.82it/s]\u001b[A\n",
            " 37%|███▋      | 748/2002 [00:59<00:57, 21.91it/s]\u001b[A\n",
            " 38%|███▊      | 751/2002 [00:59<01:03, 19.63it/s]\u001b[A\n",
            " 38%|███▊      | 756/2002 [00:59<01:05, 19.09it/s]\u001b[A\n",
            " 38%|███▊      | 759/2002 [00:59<01:09, 17.98it/s]\u001b[A\n",
            " 38%|███▊      | 764/2002 [00:59<00:56, 21.94it/s]\u001b[A\n",
            " 38%|███▊      | 769/2002 [01:00<01:18, 15.68it/s]\u001b[A\n",
            " 39%|███▊      | 772/2002 [01:00<01:14, 16.53it/s]\u001b[A\n",
            " 39%|███▉      | 776/2002 [01:00<01:04, 18.87it/s]\u001b[A\n",
            " 39%|███▉      | 779/2002 [01:01<01:39, 12.23it/s]\u001b[A\n",
            " 39%|███▉      | 782/2002 [01:01<01:35, 12.83it/s]\u001b[A\n",
            " 39%|███▉      | 785/2002 [01:01<01:42, 11.82it/s]\u001b[A\n",
            " 39%|███▉      | 787/2002 [01:01<01:30, 13.46it/s]\u001b[A\n",
            " 40%|███▉      | 792/2002 [01:01<01:13, 16.49it/s]\u001b[A\n",
            " 40%|███▉      | 795/2002 [01:02<01:07, 17.82it/s]\u001b[A\n",
            " 40%|███▉      | 800/2002 [01:02<01:02, 19.11it/s]\u001b[A\n",
            " 40%|████      | 803/2002 [01:02<01:01, 19.58it/s]\u001b[A\n",
            " 40%|████      | 806/2002 [01:02<01:00, 19.72it/s]\u001b[A\n",
            " 40%|████      | 809/2002 [01:02<00:55, 21.65it/s]\u001b[A\n",
            " 41%|████      | 812/2002 [01:02<01:15, 15.85it/s]\u001b[A\n",
            " 41%|████      | 814/2002 [01:03<01:39, 11.97it/s]\u001b[A\n",
            " 41%|████      | 818/2002 [01:03<01:21, 14.59it/s]\u001b[A\n",
            " 41%|████      | 820/2002 [01:03<02:15,  8.73it/s]\u001b[A\n",
            " 41%|████      | 822/2002 [01:03<01:57, 10.04it/s]\u001b[A\n",
            " 41%|████      | 824/2002 [01:04<01:48, 10.85it/s]\u001b[A\n",
            " 41%|████▏     | 826/2002 [01:04<01:40, 11.65it/s]\u001b[A\n",
            " 41%|████▏     | 830/2002 [01:04<01:21, 14.44it/s]\u001b[A\n",
            " 42%|████▏     | 833/2002 [01:04<01:20, 14.60it/s]\u001b[A\n",
            " 42%|████▏     | 838/2002 [01:04<01:08, 17.01it/s]\u001b[A\n",
            " 42%|████▏     | 841/2002 [01:04<01:12, 15.95it/s]\u001b[A\n",
            " 42%|████▏     | 843/2002 [01:05<01:32, 12.54it/s]\u001b[A\n",
            " 42%|████▏     | 847/2002 [01:05<01:14, 15.47it/s]\u001b[A\n",
            " 42%|████▏     | 850/2002 [01:06<02:21,  8.16it/s]\u001b[A\n",
            " 43%|████▎     | 852/2002 [01:06<02:08,  8.96it/s]\u001b[A\n",
            " 43%|████▎     | 854/2002 [01:06<01:48, 10.58it/s]\u001b[A\n",
            " 43%|████▎     | 856/2002 [01:06<02:01,  9.39it/s]\u001b[A\n",
            " 43%|████▎     | 858/2002 [01:06<01:52, 10.12it/s]\u001b[A\n",
            " 43%|████▎     | 861/2002 [01:06<01:31, 12.48it/s]\u001b[A\n",
            " 43%|████▎     | 864/2002 [01:07<01:15, 15.10it/s]\u001b[A\n",
            " 43%|████▎     | 868/2002 [01:07<01:01, 18.51it/s]\u001b[A\n",
            " 44%|████▎     | 872/2002 [01:07<01:01, 18.38it/s]\u001b[A\n",
            " 44%|████▎     | 875/2002 [01:07<01:06, 16.85it/s]\u001b[A\n",
            " 44%|████▍     | 878/2002 [01:07<01:02, 18.10it/s]\u001b[A\n",
            " 44%|████▍     | 881/2002 [01:08<01:21, 13.81it/s]\u001b[A\n",
            " 44%|████▍     | 883/2002 [01:08<01:16, 14.60it/s]\u001b[A\n",
            " 44%|████▍     | 886/2002 [01:08<01:13, 15.19it/s]\u001b[A\n",
            " 44%|████▍     | 889/2002 [01:08<01:02, 17.69it/s]\u001b[A\n",
            " 45%|████▍     | 892/2002 [01:08<01:07, 16.43it/s]\u001b[A\n",
            " 45%|████▍     | 894/2002 [01:08<01:11, 15.51it/s]\u001b[A\n",
            " 45%|████▍     | 897/2002 [01:08<01:04, 17.24it/s]\u001b[A\n",
            " 45%|████▌     | 901/2002 [01:09<00:53, 20.55it/s]\u001b[A\n",
            " 45%|████▌     | 904/2002 [01:09<01:07, 16.15it/s]\u001b[A\n",
            " 45%|████▌     | 907/2002 [01:09<01:00, 18.01it/s]\u001b[A\n",
            " 45%|████▌     | 910/2002 [01:09<01:01, 17.64it/s]\u001b[A\n",
            " 46%|████▌     | 913/2002 [01:09<00:57, 19.03it/s]\u001b[A\n",
            " 46%|████▌     | 916/2002 [01:09<01:01, 17.68it/s]\u001b[A\n",
            " 46%|████▌     | 918/2002 [01:10<01:13, 14.79it/s]\u001b[A\n",
            " 46%|████▌     | 921/2002 [01:10<01:06, 16.17it/s]\u001b[A\n",
            " 46%|████▌     | 925/2002 [01:10<00:56, 19.03it/s]\u001b[A\n",
            " 46%|████▋     | 928/2002 [01:10<00:52, 20.62it/s]\u001b[A\n",
            " 47%|████▋     | 936/2002 [01:10<00:41, 25.74it/s]\u001b[A\n",
            " 47%|████▋     | 940/2002 [01:10<00:49, 21.35it/s]\u001b[A\n",
            " 47%|████▋     | 943/2002 [01:11<01:07, 15.77it/s]\u001b[A\n",
            " 47%|████▋     | 946/2002 [01:11<01:59,  8.82it/s]\u001b[A\n",
            " 47%|████▋     | 948/2002 [01:12<02:38,  6.64it/s]\u001b[A\n",
            " 48%|████▊     | 955/2002 [01:12<01:55,  9.05it/s]\u001b[A\n",
            " 48%|████▊     | 958/2002 [01:12<01:35, 10.94it/s]\u001b[A\n",
            " 48%|████▊     | 961/2002 [01:12<01:29, 11.58it/s]\u001b[A\n",
            " 48%|████▊     | 964/2002 [01:13<01:23, 12.47it/s]\u001b[A\n",
            " 48%|████▊     | 969/2002 [01:13<01:05, 15.83it/s]\u001b[A\n",
            " 49%|████▊     | 972/2002 [01:13<00:58, 17.67it/s]\u001b[A\n",
            " 49%|████▊     | 975/2002 [01:13<01:22, 12.43it/s]\u001b[A\n",
            " 49%|████▉     | 978/2002 [01:13<01:10, 14.47it/s]\u001b[A\n",
            " 49%|████▉     | 984/2002 [01:14<01:01, 16.67it/s]\u001b[A\n",
            " 49%|████▉     | 987/2002 [01:14<01:08, 14.85it/s]\u001b[A\n",
            " 50%|████▉     | 992/2002 [01:14<01:01, 16.49it/s]\u001b[A\n",
            " 50%|████▉     | 996/2002 [01:14<00:52, 19.32it/s]\u001b[A\n",
            " 50%|████▉     | 999/2002 [01:14<01:03, 15.92it/s]\u001b[A\n",
            " 50%|█████     | 1002/2002 [01:15<00:54, 18.50it/s]\u001b[A\n",
            " 50%|█████     | 1005/2002 [01:15<00:51, 19.53it/s]\u001b[A\n",
            " 50%|█████     | 1009/2002 [01:15<00:45, 21.84it/s]\u001b[A\n",
            " 51%|█████     | 1012/2002 [01:15<00:44, 22.38it/s]\u001b[A\n",
            " 51%|█████     | 1015/2002 [01:15<01:02, 15.70it/s]\u001b[A\n",
            " 51%|█████     | 1018/2002 [01:15<00:54, 18.07it/s]\u001b[A\n",
            " 51%|█████     | 1021/2002 [01:16<00:54, 17.90it/s]\u001b[A\n",
            " 51%|█████     | 1025/2002 [01:16<00:54, 17.80it/s]\u001b[A\n",
            " 51%|█████▏    | 1030/2002 [01:16<00:45, 21.24it/s]\u001b[A\n",
            " 52%|█████▏    | 1034/2002 [01:16<00:40, 24.12it/s]\u001b[A\n",
            " 52%|█████▏    | 1037/2002 [01:16<00:56, 17.03it/s]\u001b[A\n",
            " 52%|█████▏    | 1040/2002 [01:17<01:03, 15.19it/s]\u001b[A\n",
            " 52%|█████▏    | 1046/2002 [01:17<00:49, 19.22it/s]\u001b[A\n",
            " 52%|█████▏    | 1050/2002 [01:17<00:51, 18.64it/s]\u001b[A\n",
            " 53%|█████▎    | 1053/2002 [01:17<00:48, 19.55it/s]\u001b[A\n",
            " 53%|█████▎    | 1056/2002 [01:17<00:46, 20.43it/s]\u001b[A\n",
            " 53%|█████▎    | 1059/2002 [01:17<00:50, 18.78it/s]\u001b[A\n",
            " 53%|█████▎    | 1062/2002 [01:17<00:49, 19.15it/s]\u001b[A\n",
            " 53%|█████▎    | 1065/2002 [01:18<00:44, 21.05it/s]\u001b[A\n",
            " 54%|█████▎    | 1072/2002 [01:18<00:35, 26.54it/s]\u001b[A\n",
            " 54%|█████▎    | 1076/2002 [01:18<00:36, 25.37it/s]\u001b[A\n",
            " 54%|█████▍    | 1080/2002 [01:18<00:49, 18.81it/s]\u001b[A\n",
            " 54%|█████▍    | 1083/2002 [01:18<00:44, 20.56it/s]\u001b[A\n",
            " 54%|█████▍    | 1086/2002 [01:19<01:05, 13.93it/s]\u001b[A\n",
            " 54%|█████▍    | 1089/2002 [01:19<01:01, 14.86it/s]\u001b[A\n",
            " 54%|█████▍    | 1091/2002 [01:19<00:56, 16.03it/s]\u001b[A\n",
            " 55%|█████▍    | 1093/2002 [01:19<00:54, 16.80it/s]\u001b[A\n",
            " 55%|█████▍    | 1097/2002 [01:19<00:45, 20.11it/s]\u001b[A\n",
            " 55%|█████▍    | 1100/2002 [01:20<01:00, 14.86it/s]\u001b[A\n",
            " 55%|█████▌    | 1103/2002 [01:20<00:55, 16.11it/s]\u001b[A\n",
            " 55%|█████▌    | 1106/2002 [01:20<00:53, 16.61it/s]\u001b[A\n",
            " 55%|█████▌    | 1109/2002 [01:20<00:47, 18.80it/s]\u001b[A\n",
            " 56%|█████▌    | 1112/2002 [01:20<01:01, 14.52it/s]\u001b[A\n",
            " 56%|█████▌    | 1114/2002 [01:20<00:56, 15.80it/s]\u001b[A\n",
            " 56%|█████▌    | 1116/2002 [01:20<00:55, 16.05it/s]\u001b[A\n",
            " 56%|█████▌    | 1118/2002 [01:21<00:53, 16.47it/s]\u001b[A\n",
            " 56%|█████▌    | 1122/2002 [01:21<00:51, 16.97it/s]\u001b[A\n",
            " 56%|█████▌    | 1124/2002 [01:21<01:04, 13.61it/s]\u001b[A\n",
            " 56%|█████▋    | 1128/2002 [01:21<00:55, 15.61it/s]\u001b[A\n",
            " 56%|█████▋    | 1131/2002 [01:21<00:50, 17.41it/s]\u001b[A\n",
            " 57%|█████▋    | 1134/2002 [01:22<00:52, 16.58it/s]\u001b[A\n",
            " 57%|█████▋    | 1138/2002 [01:22<00:44, 19.58it/s]\u001b[A\n",
            " 57%|█████▋    | 1141/2002 [01:22<00:39, 21.79it/s]\u001b[A\n",
            " 57%|█████▋    | 1144/2002 [01:22<00:36, 23.20it/s]\u001b[A\n",
            " 57%|█████▋    | 1147/2002 [01:22<00:52, 16.28it/s]\u001b[A\n",
            " 57%|█████▋    | 1150/2002 [01:22<00:46, 18.44it/s]\u001b[A\n",
            " 58%|█████▊    | 1153/2002 [01:22<00:48, 17.51it/s]\u001b[A\n",
            " 58%|█████▊    | 1156/2002 [01:23<01:35,  8.84it/s]\u001b[A\n",
            " 58%|█████▊    | 1159/2002 [01:23<01:22, 10.16it/s]\u001b[A\n",
            " 58%|█████▊    | 1161/2002 [01:24<01:24, 10.01it/s]\u001b[A\n",
            " 58%|█████▊    | 1164/2002 [01:24<01:07, 12.47it/s]\u001b[A\n",
            " 58%|█████▊    | 1167/2002 [01:24<00:57, 14.49it/s]\u001b[A\n",
            " 58%|█████▊    | 1169/2002 [01:24<00:54, 15.40it/s]\u001b[A\n",
            " 58%|█████▊    | 1171/2002 [01:24<01:09, 11.91it/s]\u001b[A\n",
            " 59%|█████▊    | 1175/2002 [01:24<01:01, 13.49it/s]\u001b[A\n",
            " 59%|█████▉    | 1179/2002 [01:25<00:49, 16.70it/s]\u001b[A\n",
            " 59%|█████▉    | 1182/2002 [01:25<00:50, 16.29it/s]\u001b[A\n",
            " 59%|█████▉    | 1185/2002 [01:25<00:48, 16.75it/s]\u001b[A\n",
            " 59%|█████▉    | 1189/2002 [01:25<00:41, 19.68it/s]\u001b[A\n",
            " 60%|█████▉    | 1192/2002 [01:25<00:41, 19.75it/s]\u001b[A\n",
            " 60%|█████▉    | 1195/2002 [01:25<00:39, 20.38it/s]\u001b[A\n",
            " 60%|█████▉    | 1199/2002 [01:26<00:46, 17.42it/s]\u001b[A\n",
            " 60%|█████▉    | 1201/2002 [01:26<00:50, 15.97it/s]\u001b[A\n",
            " 60%|██████    | 1203/2002 [01:26<00:47, 16.86it/s]\u001b[A\n",
            " 60%|██████    | 1206/2002 [01:26<00:43, 18.50it/s]\u001b[A\n",
            " 60%|██████    | 1208/2002 [01:26<00:43, 18.45it/s]\u001b[A\n",
            " 60%|██████    | 1211/2002 [01:26<00:43, 18.38it/s]\u001b[A\n",
            " 61%|██████    | 1213/2002 [01:26<00:44, 17.63it/s]\u001b[A\n",
            " 61%|██████    | 1215/2002 [01:26<00:45, 17.31it/s]\u001b[A\n",
            " 61%|██████    | 1221/2002 [01:27<00:37, 20.60it/s]\u001b[A\n",
            " 61%|██████    | 1224/2002 [01:27<00:47, 16.36it/s]\u001b[A\n",
            " 61%|██████▏   | 1227/2002 [01:27<00:46, 16.62it/s]\u001b[A\n",
            " 61%|██████▏   | 1229/2002 [01:27<00:59, 12.92it/s]\u001b[A\n",
            " 62%|██████▏   | 1234/2002 [01:27<00:49, 15.64it/s]\u001b[A\n",
            " 62%|██████▏   | 1239/2002 [01:28<00:40, 18.99it/s]\u001b[A\n",
            " 62%|██████▏   | 1243/2002 [01:28<00:33, 22.36it/s]\u001b[A\n",
            " 62%|██████▏   | 1246/2002 [01:28<00:46, 16.21it/s]\u001b[A\n",
            " 62%|██████▏   | 1249/2002 [01:28<00:42, 17.65it/s]\u001b[A\n",
            " 63%|██████▎   | 1252/2002 [01:28<00:42, 17.66it/s]\u001b[A\n",
            " 63%|██████▎   | 1255/2002 [01:29<00:54, 13.66it/s]\u001b[A\n",
            " 63%|██████▎   | 1257/2002 [01:29<01:03, 11.74it/s]\u001b[A\n",
            " 63%|██████▎   | 1259/2002 [01:29<01:08, 10.86it/s]\u001b[A\n",
            " 63%|██████▎   | 1261/2002 [01:29<01:18,  9.42it/s]\u001b[A\n",
            " 63%|██████▎   | 1264/2002 [01:30<01:07, 10.98it/s]\u001b[A\n",
            " 63%|██████▎   | 1268/2002 [01:30<00:53, 13.78it/s]\u001b[A\n",
            " 64%|██████▎   | 1273/2002 [01:30<00:43, 16.88it/s]\u001b[A\n",
            " 64%|██████▍   | 1277/2002 [01:30<00:37, 19.57it/s]\u001b[A\n",
            " 64%|██████▍   | 1280/2002 [01:30<00:41, 17.55it/s]\u001b[A\n",
            " 64%|██████▍   | 1283/2002 [01:30<00:38, 18.78it/s]\u001b[A\n",
            " 64%|██████▍   | 1286/2002 [01:30<00:36, 19.80it/s]\u001b[A\n",
            " 64%|██████▍   | 1289/2002 [01:31<00:34, 20.81it/s]\u001b[A\n",
            " 65%|██████▍   | 1292/2002 [01:31<00:45, 15.73it/s]\u001b[A\n",
            " 65%|██████▍   | 1297/2002 [01:31<00:38, 18.53it/s]\u001b[A\n",
            " 65%|██████▍   | 1300/2002 [01:31<00:40, 17.50it/s]\u001b[A\n",
            " 65%|██████▌   | 1304/2002 [01:31<00:35, 19.50it/s]\u001b[A\n",
            " 65%|██████▌   | 1307/2002 [01:32<00:43, 16.11it/s]\u001b[A\n",
            " 65%|██████▌   | 1309/2002 [01:32<00:48, 14.39it/s]\u001b[A\n",
            " 65%|██████▌   | 1311/2002 [01:32<01:42,  6.73it/s]\u001b[A\n",
            " 66%|██████▌   | 1315/2002 [01:33<01:20,  8.49it/s]\u001b[A\n",
            " 66%|██████▌   | 1319/2002 [01:33<01:01, 11.10it/s]\u001b[A\n",
            " 66%|██████▌   | 1322/2002 [01:33<01:03, 10.72it/s]\u001b[A\n",
            " 66%|██████▌   | 1326/2002 [01:33<00:49, 13.54it/s]\u001b[A\n",
            " 66%|██████▋   | 1329/2002 [01:33<00:44, 15.23it/s]\u001b[A\n",
            " 67%|██████▋   | 1332/2002 [01:34<00:56, 11.76it/s]\u001b[A\n",
            " 67%|██████▋   | 1337/2002 [01:34<00:43, 15.18it/s]\u001b[A\n",
            " 67%|██████▋   | 1340/2002 [01:34<00:48, 13.68it/s]\u001b[A\n",
            " 67%|██████▋   | 1343/2002 [01:34<00:43, 15.13it/s]\u001b[A\n",
            " 67%|██████▋   | 1346/2002 [01:34<00:37, 17.28it/s]\u001b[A\n",
            " 67%|██████▋   | 1349/2002 [01:35<00:48, 13.56it/s]\u001b[A\n",
            " 67%|██████▋   | 1351/2002 [01:35<00:44, 14.70it/s]\u001b[A\n",
            " 68%|██████▊   | 1353/2002 [01:35<00:42, 15.27it/s]\u001b[A\n",
            " 68%|██████▊   | 1356/2002 [01:35<00:41, 15.49it/s]\u001b[A\n",
            " 68%|██████▊   | 1358/2002 [01:35<00:47, 13.47it/s]\u001b[A\n",
            " 68%|██████▊   | 1361/2002 [01:35<00:42, 14.93it/s]\u001b[A\n",
            " 68%|██████▊   | 1364/2002 [01:36<00:40, 15.94it/s]\u001b[A\n",
            " 68%|██████▊   | 1366/2002 [01:42<10:05,  1.05it/s]\u001b[A\n",
            " 68%|██████▊   | 1369/2002 [01:42<07:10,  1.47it/s]\u001b[A\n",
            " 69%|██████▊   | 1372/2002 [01:42<05:08,  2.04it/s]\u001b[A\n",
            " 69%|██████▉   | 1377/2002 [01:42<03:42,  2.81it/s]\u001b[A\n",
            " 69%|██████▉   | 1380/2002 [01:42<02:43,  3.81it/s]\u001b[A\n",
            " 69%|██████▉   | 1383/2002 [01:42<02:04,  4.98it/s]\u001b[A\n",
            " 69%|██████▉   | 1387/2002 [01:43<01:32,  6.66it/s]\u001b[A\n",
            " 69%|██████▉   | 1390/2002 [01:43<01:22,  7.45it/s]\u001b[A\n",
            " 70%|██████▉   | 1393/2002 [01:43<01:06,  9.15it/s]\u001b[A\n",
            " 70%|██████▉   | 1397/2002 [01:43<00:52, 11.47it/s]\u001b[A\n",
            " 70%|██████▉   | 1400/2002 [01:43<00:43, 13.98it/s]\u001b[A\n",
            " 70%|███████   | 1403/2002 [01:44<01:05,  9.17it/s]\u001b[A\n",
            " 70%|███████   | 1406/2002 [01:44<00:52, 11.38it/s]\u001b[A\n",
            " 70%|███████   | 1410/2002 [01:44<00:40, 14.46it/s]\u001b[A\n",
            " 71%|███████   | 1413/2002 [01:44<00:35, 16.72it/s]\u001b[A\n",
            " 71%|███████   | 1416/2002 [01:44<00:32, 17.93it/s]\u001b[A\n",
            " 71%|███████   | 1419/2002 [01:45<00:37, 15.62it/s]\u001b[A\n",
            " 71%|███████   | 1422/2002 [01:45<00:37, 15.64it/s]\u001b[A\n",
            " 71%|███████   | 1425/2002 [01:45<00:33, 17.01it/s]\u001b[A\n",
            " 71%|███████▏  | 1428/2002 [01:45<00:37, 15.14it/s]\u001b[A\n",
            " 71%|███████▏  | 1430/2002 [01:45<00:38, 14.79it/s]\u001b[A\n",
            " 72%|███████▏  | 1432/2002 [01:45<00:42, 13.50it/s]\u001b[A\n",
            " 72%|███████▏  | 1436/2002 [01:46<00:39, 14.45it/s]\u001b[A\n",
            " 72%|███████▏  | 1438/2002 [01:46<00:39, 14.35it/s]\u001b[A\n",
            " 72%|███████▏  | 1440/2002 [01:46<00:54, 10.29it/s]\u001b[A\n",
            " 72%|███████▏  | 1444/2002 [01:46<00:43, 12.94it/s]\u001b[A\n",
            " 72%|███████▏  | 1447/2002 [01:46<00:36, 15.02it/s]\u001b[A\n",
            " 72%|███████▏  | 1450/2002 [01:47<00:39, 13.99it/s]\u001b[A\n",
            " 73%|███████▎  | 1452/2002 [01:47<00:37, 14.53it/s]\u001b[A\n",
            " 73%|███████▎  | 1454/2002 [01:47<00:36, 14.98it/s]\u001b[A\n",
            " 73%|███████▎  | 1460/2002 [01:47<00:28, 18.85it/s]\u001b[A\n",
            " 73%|███████▎  | 1464/2002 [01:47<00:27, 19.33it/s]\u001b[A\n",
            " 73%|███████▎  | 1467/2002 [01:48<00:55,  9.65it/s]\u001b[A\n",
            " 73%|███████▎  | 1469/2002 [01:48<01:09,  7.72it/s]\u001b[A\n",
            " 74%|███████▎  | 1473/2002 [01:48<00:54,  9.74it/s]\u001b[A\n",
            " 74%|███████▎  | 1476/2002 [01:49<00:48, 10.81it/s]\u001b[A\n",
            " 74%|███████▍  | 1478/2002 [01:49<00:53,  9.87it/s]\u001b[A\n",
            " 74%|███████▍  | 1482/2002 [01:49<00:42, 12.23it/s]\u001b[A\n",
            " 74%|███████▍  | 1484/2002 [01:49<00:40, 12.92it/s]\u001b[A\n",
            " 74%|███████▍  | 1486/2002 [01:49<00:37, 13.75it/s]\u001b[A\n",
            " 75%|███████▍  | 1492/2002 [01:50<00:33, 15.37it/s]\u001b[A\n",
            " 75%|███████▍  | 1494/2002 [01:50<00:33, 15.04it/s]\u001b[A\n",
            " 75%|███████▍  | 1496/2002 [01:50<00:34, 14.58it/s]\u001b[A\n",
            " 75%|███████▍  | 1498/2002 [01:50<00:32, 15.45it/s]\u001b[A\n",
            " 75%|███████▌  | 1502/2002 [01:50<00:30, 16.26it/s]\u001b[A\n",
            " 75%|███████▌  | 1505/2002 [01:50<00:28, 17.71it/s]\u001b[A\n",
            " 75%|███████▌  | 1507/2002 [01:50<00:28, 17.35it/s]\u001b[A\n",
            " 75%|███████▌  | 1509/2002 [01:51<00:32, 15.23it/s]\u001b[A\n",
            " 76%|███████▌  | 1512/2002 [01:51<00:35, 13.99it/s]\u001b[A\n",
            " 76%|███████▌  | 1514/2002 [01:51<00:32, 15.02it/s]\u001b[A\n",
            " 76%|███████▌  | 1516/2002 [01:51<00:30, 15.72it/s]\u001b[A\n",
            " 76%|███████▌  | 1519/2002 [01:51<00:29, 16.30it/s]\u001b[A\n",
            " 76%|███████▌  | 1521/2002 [01:51<00:27, 17.22it/s]\u001b[A\n",
            " 76%|███████▌  | 1524/2002 [01:52<00:31, 15.21it/s]\u001b[A\n",
            " 76%|███████▋  | 1528/2002 [01:52<00:26, 18.00it/s]\u001b[A\n",
            " 76%|███████▋  | 1531/2002 [01:52<00:26, 17.67it/s]\u001b[A\n",
            " 77%|███████▋  | 1535/2002 [01:52<00:22, 20.96it/s]\u001b[A\n",
            " 77%|███████▋  | 1539/2002 [01:52<00:26, 17.28it/s]\u001b[A\n",
            " 77%|███████▋  | 1544/2002 [01:52<00:22, 20.08it/s]\u001b[A\n",
            " 77%|███████▋  | 1551/2002 [01:53<00:18, 24.22it/s]\u001b[A\n",
            " 78%|███████▊  | 1555/2002 [01:53<00:18, 24.68it/s]\u001b[A\n",
            " 78%|███████▊  | 1558/2002 [01:53<00:25, 17.48it/s]\u001b[A\n",
            " 78%|███████▊  | 1561/2002 [01:53<00:25, 16.97it/s]\u001b[A\n",
            " 78%|███████▊  | 1564/2002 [01:53<00:22, 19.28it/s]\u001b[A\n",
            " 78%|███████▊  | 1567/2002 [01:54<00:23, 18.45it/s]\u001b[A\n",
            " 78%|███████▊  | 1570/2002 [01:54<00:33, 12.72it/s]\u001b[A\n",
            " 79%|███████▊  | 1572/2002 [01:54<00:32, 13.21it/s]\u001b[A\n",
            " 79%|███████▊  | 1574/2002 [01:54<00:32, 13.13it/s]\u001b[A\n",
            " 79%|███████▊  | 1576/2002 [01:54<00:37, 11.39it/s]\u001b[A\n",
            " 79%|███████▉  | 1579/2002 [01:55<00:31, 13.47it/s]\u001b[A\n",
            " 79%|███████▉  | 1581/2002 [01:55<00:31, 13.40it/s]\u001b[A\n",
            " 79%|███████▉  | 1583/2002 [01:55<00:29, 14.17it/s]\u001b[A\n",
            " 79%|███████▉  | 1586/2002 [01:55<00:29, 13.96it/s]\u001b[A\n",
            " 79%|███████▉  | 1590/2002 [01:55<00:24, 16.64it/s]\u001b[A\n",
            " 80%|███████▉  | 1592/2002 [01:55<00:27, 14.92it/s]\u001b[A\n",
            " 80%|███████▉  | 1596/2002 [01:56<00:35, 11.52it/s]\u001b[A\n",
            " 80%|███████▉  | 1599/2002 [01:56<00:31, 12.62it/s]\u001b[A\n",
            " 80%|████████  | 1602/2002 [01:56<00:27, 14.64it/s]\u001b[A\n",
            " 80%|████████  | 1605/2002 [01:56<00:23, 16.60it/s]\u001b[A\n",
            " 80%|████████  | 1607/2002 [01:57<00:30, 12.89it/s]\u001b[A\n",
            " 80%|████████  | 1609/2002 [01:57<00:34, 11.36it/s]\u001b[A\n",
            " 81%|████████  | 1615/2002 [01:57<00:27, 14.21it/s]\u001b[A\n",
            " 81%|████████  | 1618/2002 [01:57<00:23, 16.50it/s]\u001b[A\n",
            " 81%|████████  | 1621/2002 [01:57<00:27, 13.76it/s]\u001b[A\n",
            " 81%|████████  | 1625/2002 [01:58<00:22, 17.05it/s]\u001b[A\n",
            " 81%|████████▏ | 1628/2002 [01:58<00:22, 16.56it/s]\u001b[A\n",
            " 82%|████████▏ | 1632/2002 [01:58<00:18, 19.53it/s]\u001b[A\n",
            " 82%|████████▏ | 1635/2002 [01:58<00:21, 17.31it/s]\u001b[A\n",
            " 82%|████████▏ | 1638/2002 [01:58<00:20, 18.02it/s]\u001b[A\n",
            " 82%|████████▏ | 1641/2002 [01:58<00:20, 17.68it/s]\u001b[A\n",
            " 82%|████████▏ | 1645/2002 [01:59<00:21, 16.69it/s]\u001b[A\n",
            " 82%|████████▏ | 1647/2002 [01:59<00:26, 13.50it/s]\u001b[A\n",
            " 83%|████████▎ | 1655/2002 [01:59<00:19, 17.82it/s]\u001b[A\n",
            " 83%|████████▎ | 1659/2002 [01:59<00:17, 19.22it/s]\u001b[A\n",
            " 83%|████████▎ | 1662/2002 [02:05<03:32,  1.60it/s]\u001b[A\n",
            " 83%|████████▎ | 1666/2002 [02:05<02:31,  2.22it/s]\u001b[A\n",
            " 83%|████████▎ | 1669/2002 [02:05<01:50,  3.02it/s]\u001b[A\n",
            " 84%|████████▎ | 1672/2002 [02:06<01:21,  4.03it/s]\u001b[A\n",
            " 84%|████████▍ | 1677/2002 [02:06<00:58,  5.52it/s]\u001b[A\n",
            " 84%|████████▍ | 1680/2002 [02:06<00:55,  5.82it/s]\u001b[A\n",
            " 84%|████████▍ | 1684/2002 [02:06<00:41,  7.60it/s]\u001b[A\n",
            " 84%|████████▍ | 1689/2002 [02:06<00:31, 10.07it/s]\u001b[A\n",
            " 85%|████████▍ | 1692/2002 [02:07<00:28, 10.78it/s]\u001b[A\n",
            " 85%|████████▍ | 1695/2002 [02:07<00:23, 12.95it/s]\u001b[A\n",
            " 85%|████████▍ | 1701/2002 [02:07<00:19, 15.06it/s]\u001b[A\n",
            " 85%|████████▌ | 1705/2002 [02:07<00:16, 18.51it/s]\u001b[A\n",
            " 85%|████████▌ | 1710/2002 [02:07<00:13, 22.32it/s]\u001b[A\n",
            " 86%|████████▌ | 1714/2002 [02:07<00:12, 22.87it/s]\u001b[A\n",
            " 86%|████████▌ | 1718/2002 [02:08<00:11, 24.08it/s]\u001b[A\n",
            " 86%|████████▌ | 1721/2002 [02:08<00:16, 17.41it/s]\u001b[A\n",
            " 86%|████████▌ | 1724/2002 [02:08<00:16, 17.03it/s]\u001b[A\n",
            " 86%|████████▋ | 1727/2002 [02:08<00:14, 18.90it/s]\u001b[A\n",
            " 87%|████████▋ | 1732/2002 [02:08<00:13, 20.28it/s]\u001b[A\n",
            " 87%|████████▋ | 1736/2002 [02:09<00:14, 18.70it/s]\u001b[A\n",
            " 87%|████████▋ | 1740/2002 [02:09<00:11, 22.13it/s]\u001b[A\n",
            " 87%|████████▋ | 1744/2002 [02:14<01:59,  2.17it/s]\u001b[A\n",
            " 87%|████████▋ | 1746/2002 [02:15<01:28,  2.90it/s]\u001b[A\n",
            " 88%|████████▊ | 1752/2002 [02:15<01:01,  4.05it/s]\u001b[A\n",
            " 88%|████████▊ | 1756/2002 [02:15<00:50,  4.90it/s]\u001b[A\n",
            " 88%|████████▊ | 1759/2002 [02:15<00:38,  6.37it/s]\u001b[A\n",
            " 88%|████████▊ | 1762/2002 [02:15<00:30,  7.93it/s]\u001b[A\n",
            " 88%|████████▊ | 1765/2002 [02:15<00:23, 10.10it/s]\u001b[A\n",
            " 88%|████████▊ | 1770/2002 [02:16<00:17, 13.07it/s]\u001b[A\n",
            " 89%|████████▊ | 1773/2002 [02:16<00:17, 13.08it/s]\u001b[A\n",
            " 89%|████████▉ | 1777/2002 [02:16<00:14, 16.02it/s]\u001b[A\n",
            " 89%|████████▉ | 1782/2002 [02:16<00:11, 19.43it/s]\u001b[A\n",
            " 89%|████████▉ | 1786/2002 [02:16<00:12, 16.70it/s]\u001b[A\n",
            " 89%|████████▉ | 1789/2002 [02:17<00:14, 14.91it/s]\u001b[A\n",
            " 90%|████████▉ | 1792/2002 [02:17<00:15, 13.98it/s]\u001b[A\n",
            " 90%|████████▉ | 1794/2002 [02:17<00:16, 12.52it/s]\u001b[A\n",
            " 90%|████████▉ | 1797/2002 [02:17<00:14, 14.25it/s]\u001b[A\n",
            " 90%|████████▉ | 1799/2002 [02:17<00:16, 12.37it/s]\u001b[A\n",
            " 90%|█████████ | 1802/2002 [02:18<00:14, 13.87it/s]\u001b[A\n",
            " 90%|█████████ | 1804/2002 [02:18<00:15, 12.87it/s]\u001b[A\n",
            " 90%|█████████ | 1806/2002 [02:18<00:14, 13.98it/s]\u001b[A\n",
            " 90%|█████████ | 1808/2002 [02:18<00:12, 15.34it/s]\u001b[A\n",
            " 90%|█████████ | 1810/2002 [02:18<00:15, 12.25it/s]\u001b[A\n",
            " 91%|█████████ | 1814/2002 [02:18<00:14, 13.08it/s]\u001b[A\n",
            " 91%|█████████ | 1816/2002 [02:19<00:12, 14.53it/s]\u001b[A\n",
            " 91%|█████████ | 1818/2002 [02:19<00:12, 15.22it/s]\u001b[A\n",
            " 91%|█████████ | 1820/2002 [02:19<00:17, 10.60it/s]\u001b[A\n",
            " 91%|█████████ | 1822/2002 [02:19<00:22,  8.07it/s]\u001b[A\n",
            " 91%|█████████ | 1825/2002 [02:20<00:17,  9.95it/s]\u001b[A\n",
            " 91%|█████████▏| 1828/2002 [02:20<00:14, 11.86it/s]\u001b[A\n",
            " 91%|█████████▏| 1831/2002 [02:20<00:12, 13.88it/s]\u001b[A\n",
            " 92%|█████████▏| 1833/2002 [02:20<00:13, 12.33it/s]\u001b[A\n",
            " 92%|█████████▏| 1836/2002 [02:20<00:12, 13.40it/s]\u001b[A\n",
            " 92%|█████████▏| 1838/2002 [02:20<00:13, 11.83it/s]\u001b[A\n",
            " 92%|█████████▏| 1841/2002 [02:21<00:11, 14.01it/s]\u001b[A\n",
            " 92%|█████████▏| 1845/2002 [02:21<00:09, 16.34it/s]\u001b[A\n",
            " 92%|█████████▏| 1848/2002 [02:21<00:11, 13.99it/s]\u001b[A\n",
            " 92%|█████████▏| 1850/2002 [02:21<00:10, 14.09it/s]\u001b[A\n",
            " 93%|█████████▎| 1852/2002 [02:21<00:12, 12.48it/s]\u001b[A\n",
            " 93%|█████████▎| 1857/2002 [02:21<00:09, 15.56it/s]\u001b[A\n",
            " 93%|█████████▎| 1860/2002 [02:22<00:10, 13.00it/s]\u001b[A\n",
            " 93%|█████████▎| 1863/2002 [02:22<00:09, 15.02it/s]\u001b[A\n",
            " 93%|█████████▎| 1866/2002 [02:22<00:07, 17.04it/s]\u001b[A\n",
            " 93%|█████████▎| 1869/2002 [02:22<00:07, 18.92it/s]\u001b[A\n",
            " 94%|█████████▎| 1872/2002 [02:22<00:09, 14.04it/s]\u001b[A\n",
            " 94%|█████████▎| 1875/2002 [02:23<00:08, 15.85it/s]\u001b[A\n",
            " 94%|█████████▍| 1880/2002 [02:23<00:06, 19.01it/s]\u001b[A\n",
            " 94%|█████████▍| 1883/2002 [02:23<00:06, 18.09it/s]\u001b[A\n",
            " 94%|█████████▍| 1886/2002 [02:23<00:06, 17.98it/s]\u001b[A\n",
            " 94%|█████████▍| 1889/2002 [02:23<00:06, 17.39it/s]\u001b[A\n",
            " 95%|█████████▍| 1893/2002 [02:23<00:05, 20.02it/s]\u001b[A\n",
            " 95%|█████████▍| 1896/2002 [02:24<00:06, 17.48it/s]\u001b[A\n",
            " 95%|█████████▍| 1899/2002 [02:24<00:05, 19.43it/s]\u001b[A\n",
            " 95%|█████████▌| 1902/2002 [02:24<00:05, 19.81it/s]\u001b[A\n",
            " 95%|█████████▌| 1905/2002 [02:24<00:04, 19.94it/s]\u001b[A\n",
            " 95%|█████████▌| 1908/2002 [02:24<00:04, 19.78it/s]\u001b[A\n",
            " 95%|█████████▌| 1911/2002 [02:24<00:05, 16.29it/s]\u001b[A\n",
            " 96%|█████████▌| 1913/2002 [02:25<00:05, 16.17it/s]\u001b[A\n",
            " 96%|█████████▌| 1915/2002 [02:25<00:05, 17.06it/s]\u001b[A\n",
            " 96%|█████████▌| 1917/2002 [02:25<00:04, 17.20it/s]\u001b[A\n",
            " 96%|█████████▌| 1920/2002 [02:25<00:04, 19.16it/s]\u001b[A\n",
            " 96%|█████████▌| 1923/2002 [02:25<00:04, 17.05it/s]\u001b[A\n",
            " 96%|█████████▋| 1929/2002 [02:25<00:03, 21.56it/s]\u001b[A\n",
            " 97%|█████████▋| 1935/2002 [02:25<00:02, 25.90it/s]\u001b[A\n",
            " 97%|█████████▋| 1939/2002 [02:26<00:02, 25.23it/s]\u001b[A\n",
            " 97%|█████████▋| 1943/2002 [02:26<00:02, 20.07it/s]\u001b[A\n",
            " 97%|█████████▋| 1948/2002 [02:26<00:02, 21.48it/s]\u001b[A\n",
            " 97%|█████████▋| 1951/2002 [02:26<00:02, 21.03it/s]\u001b[A\n",
            " 98%|█████████▊| 1955/2002 [02:26<00:01, 24.43it/s]\u001b[A\n",
            " 98%|█████████▊| 1958/2002 [02:27<00:02, 18.09it/s]\u001b[A\n",
            " 98%|█████████▊| 1961/2002 [02:27<00:02, 18.83it/s]\u001b[A\n",
            " 98%|█████████▊| 1964/2002 [02:27<00:04,  9.30it/s]\u001b[A\n",
            " 98%|█████████▊| 1967/2002 [02:28<00:03, 11.13it/s]\u001b[A\n",
            " 98%|█████████▊| 1969/2002 [02:28<00:02, 12.40it/s]\u001b[A\n",
            " 98%|█████████▊| 1971/2002 [02:28<00:02, 11.21it/s]\u001b[A\n",
            " 99%|█████████▉| 1979/2002 [02:28<00:01, 14.71it/s]\u001b[A\n",
            " 99%|█████████▉| 1983/2002 [02:28<00:01, 17.35it/s]\u001b[A\n",
            " 99%|█████████▉| 1986/2002 [02:28<00:00, 19.69it/s]\u001b[A\n",
            " 99%|█████████▉| 1989/2002 [02:29<00:00, 13.95it/s]\u001b[A\n",
            "100%|█████████▉| 1992/2002 [02:29<00:00, 15.42it/s]\u001b[A\n",
            "100%|█████████▉| 1996/2002 [02:29<00:00, 18.13it/s]\u001b[A\n",
            "100%|█████████▉| 1999/2002 [02:29<00:00, 14.88it/s]\u001b[A\n",
            "100%|██████████| 2002/2002 [02:29<00:00, 13.36it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "NYTimes Description:\n",
            "The New York Times: Find breaking news, multimedia, reviews & opinion on Washington, business, sports, movies, travel, books, jobs, education, real estate, cars & more at nytimes.com.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzdLW0Aya_iy",
        "colab_type": "text"
      },
      "source": [
        "## Let's get our value desciptions!\n",
        "\n",
        "Once you have this working, call the *get_descriptions_from_data* for val to get *val_descriptions*. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPuUgA39bO3f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "684e01c3-3633-4c46-ce3e-5afae25a5935"
      },
      "source": [
        "val_descriptions = get_descriptions_from_data(val_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/309 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|▏         | 4/309 [00:00<00:23, 13.05it/s]\u001b[A\n",
            "  2%|▏         | 7/309 [00:00<00:23, 12.68it/s]\u001b[A\n",
            "  3%|▎         | 9/309 [00:00<00:25, 11.76it/s]\u001b[A\n",
            "  4%|▍         | 12/309 [00:00<00:22, 13.49it/s]\u001b[A\n",
            "  5%|▍         | 14/309 [00:01<00:25, 11.65it/s]\u001b[A\n",
            "  5%|▌         | 16/309 [00:01<00:34,  8.40it/s]\u001b[A\n",
            "  6%|▌         | 18/309 [00:01<00:29,  9.80it/s]\u001b[A\n",
            "  6%|▋         | 20/309 [00:01<00:25, 11.13it/s]\u001b[A\n",
            "  7%|▋         | 22/309 [00:02<00:31,  8.99it/s]\u001b[A\n",
            "  8%|▊         | 25/309 [00:02<00:25, 11.30it/s]\u001b[A\n",
            " 10%|▉         | 30/309 [00:02<00:19, 14.44it/s]\u001b[A\n",
            " 11%|█         | 34/309 [00:02<00:16, 17.14it/s]\u001b[A\n",
            " 13%|█▎        | 39/309 [00:02<00:15, 17.90it/s]\u001b[A\n",
            " 14%|█▎        | 42/309 [00:02<00:14, 18.22it/s]\u001b[A\n",
            " 15%|█▍        | 45/309 [00:03<00:14, 17.98it/s]\u001b[A\n",
            " 16%|█▌        | 48/309 [00:03<00:20, 12.77it/s]\u001b[A\n",
            " 17%|█▋        | 51/309 [00:03<00:21, 12.25it/s]\u001b[A\n",
            " 17%|█▋        | 53/309 [00:03<00:24, 10.44it/s]\u001b[A\n",
            " 18%|█▊        | 55/309 [00:04<00:31,  7.98it/s]\u001b[A\n",
            " 18%|█▊        | 57/309 [00:04<00:29,  8.56it/s]\u001b[A\n",
            " 19%|█▉        | 60/309 [00:04<00:23, 10.59it/s]\u001b[A\n",
            " 21%|██▏       | 66/309 [00:04<00:18, 13.45it/s]\u001b[A\n",
            " 22%|██▏       | 69/309 [00:05<00:16, 14.22it/s]\u001b[A\n",
            " 23%|██▎       | 71/309 [00:05<00:16, 14.65it/s]\u001b[A\n",
            " 24%|██▎       | 73/309 [00:05<00:15, 15.68it/s]\u001b[A\n",
            " 25%|██▍       | 77/309 [00:05<00:15, 14.96it/s]\u001b[A\n",
            " 26%|██▌       | 79/309 [00:05<00:15, 14.45it/s]\u001b[A\n",
            " 26%|██▌       | 81/309 [00:05<00:15, 14.68it/s]\u001b[A\n",
            " 27%|██▋       | 83/309 [00:06<00:16, 13.34it/s]\u001b[A\n",
            " 28%|██▊       | 87/309 [00:06<00:15, 14.69it/s]\u001b[A\n",
            " 29%|██▉       | 89/309 [00:06<00:13, 15.83it/s]\u001b[A\n",
            " 29%|██▉       | 91/309 [00:06<00:12, 16.83it/s]\u001b[A\n",
            " 31%|███       | 95/309 [00:06<00:10, 20.33it/s]\u001b[A\n",
            " 32%|███▏      | 98/309 [00:06<00:09, 21.37it/s]\u001b[A\n",
            " 33%|███▎      | 101/309 [00:06<00:09, 20.92it/s]\u001b[A\n",
            " 34%|███▎      | 104/309 [00:07<00:13, 15.15it/s]\u001b[A\n",
            " 34%|███▍      | 106/309 [00:07<00:16, 12.49it/s]\u001b[A\n",
            " 35%|███▍      | 108/309 [00:07<00:20,  9.69it/s]\u001b[A\n",
            " 36%|███▌      | 110/309 [00:07<00:20,  9.59it/s]\u001b[A\n",
            " 37%|███▋      | 113/309 [00:08<00:17, 11.37it/s]\u001b[A\n",
            " 38%|███▊      | 116/309 [00:08<00:14, 13.70it/s]\u001b[A\n",
            " 39%|███▊      | 119/309 [00:08<00:11, 16.07it/s]\u001b[A\n",
            " 39%|███▉      | 122/309 [00:08<00:15, 12.46it/s]\u001b[A\n",
            " 41%|████      | 127/309 [00:08<00:11, 15.75it/s]\u001b[A\n",
            " 42%|████▏     | 130/309 [00:08<00:11, 15.81it/s]\u001b[A\n",
            " 43%|████▎     | 133/309 [00:09<00:14, 12.00it/s]\u001b[A\n",
            " 44%|████▎     | 135/309 [00:09<00:13, 13.38it/s]\u001b[A\n",
            " 44%|████▍     | 137/309 [00:09<00:13, 13.18it/s]\u001b[A\n",
            " 45%|████▍     | 139/309 [00:09<00:15, 10.80it/s]\u001b[A\n",
            " 46%|████▋     | 143/309 [00:09<00:12, 13.11it/s]\u001b[A\n",
            " 47%|████▋     | 145/309 [00:10<00:13, 12.31it/s]\u001b[A\n",
            " 48%|████▊     | 148/309 [00:10<00:10, 14.94it/s]\u001b[A\n",
            " 49%|████▊     | 150/309 [00:10<00:10, 14.92it/s]\u001b[A\n",
            " 49%|████▉     | 152/309 [00:10<00:16,  9.69it/s]\u001b[A\n",
            " 50%|█████     | 156/309 [00:10<00:12, 12.27it/s]\u001b[A\n",
            " 51%|█████     | 158/309 [00:11<00:11, 13.39it/s]\u001b[A\n",
            " 52%|█████▏    | 162/309 [00:11<00:08, 16.70it/s]\u001b[A\n",
            " 53%|█████▎    | 165/309 [00:11<00:11, 12.21it/s]\u001b[A\n",
            " 54%|█████▍    | 167/309 [00:11<00:12, 11.12it/s]\u001b[A\n",
            " 55%|█████▌    | 170/309 [00:11<00:10, 13.08it/s]\u001b[A\n",
            " 56%|█████▌    | 172/309 [00:12<00:15,  8.76it/s]\u001b[A\n",
            " 57%|█████▋    | 175/309 [00:12<00:12, 10.57it/s]\u001b[A\n",
            " 57%|█████▋    | 177/309 [00:12<00:10, 12.04it/s]\u001b[A\n",
            " 58%|█████▊    | 180/309 [00:12<00:10, 12.49it/s]\u001b[A\n",
            " 59%|█████▉    | 183/309 [00:12<00:09, 13.11it/s]\u001b[A\n",
            " 60%|██████    | 186/309 [00:13<00:08, 14.74it/s]\u001b[A\n",
            " 61%|██████    | 189/309 [00:13<00:07, 15.94it/s]\u001b[A\n",
            " 62%|██████▏   | 191/309 [00:13<00:09, 12.70it/s]\u001b[A\n",
            " 63%|██████▎   | 196/309 [00:13<00:07, 16.05it/s]\u001b[A\n",
            " 64%|██████▍   | 199/309 [00:13<00:07, 13.96it/s]\u001b[A\n",
            " 65%|██████▌   | 201/309 [00:14<00:09, 11.28it/s]\u001b[A\n",
            " 66%|██████▌   | 203/309 [00:14<00:08, 12.71it/s]\u001b[A\n",
            " 66%|██████▋   | 205/309 [00:14<00:07, 13.98it/s]\u001b[A\n",
            " 67%|██████▋   | 207/309 [00:14<00:08, 12.03it/s]\u001b[A\n",
            " 68%|██████▊   | 210/309 [00:14<00:06, 14.29it/s]\u001b[A\n",
            " 69%|██████▊   | 212/309 [00:14<00:08, 12.12it/s]\u001b[A\n",
            " 70%|██████▉   | 215/309 [00:15<00:08, 11.65it/s]\u001b[A\n",
            " 71%|███████   | 220/309 [00:15<00:05, 14.94it/s]\u001b[A\n",
            " 72%|███████▏  | 223/309 [00:15<00:09,  9.41it/s]\u001b[A\n",
            " 73%|███████▎  | 227/309 [00:16<00:06, 11.84it/s]\u001b[A\n",
            " 74%|███████▍  | 230/309 [00:16<00:05, 14.44it/s]\u001b[A\n",
            " 75%|███████▌  | 233/309 [00:16<00:07, 10.82it/s]\u001b[A\n",
            " 76%|███████▋  | 236/309 [00:16<00:06, 11.12it/s]\u001b[A\n",
            " 77%|███████▋  | 238/309 [00:17<00:07,  9.51it/s]\u001b[A\n",
            " 78%|███████▊  | 241/309 [00:17<00:07,  9.71it/s]\u001b[A\n",
            " 79%|███████▊  | 243/309 [00:17<00:05, 11.08it/s]\u001b[A\n",
            " 79%|███████▉  | 245/309 [00:17<00:07,  8.36it/s]\u001b[A\n",
            " 81%|████████  | 249/309 [00:18<00:05, 10.66it/s]\u001b[A\n",
            " 82%|████████▏ | 252/309 [00:18<00:04, 12.84it/s]\u001b[A\n",
            " 82%|████████▏ | 254/309 [00:18<00:04, 13.32it/s]\u001b[A\n",
            " 84%|████████▍ | 259/309 [00:18<00:03, 12.93it/s]\u001b[A\n",
            " 84%|████████▍ | 261/309 [00:18<00:04, 11.30it/s]\u001b[A\n",
            " 85%|████████▌ | 263/309 [00:19<00:03, 12.75it/s]\u001b[A\n",
            " 86%|████████▌ | 265/309 [00:19<00:03, 12.00it/s]\u001b[A\n",
            " 86%|████████▋ | 267/309 [00:19<00:03, 13.56it/s]\u001b[A\n",
            " 87%|████████▋ | 270/309 [00:19<00:02, 15.69it/s]\u001b[A\n",
            " 88%|████████▊ | 272/309 [00:19<00:03, 11.35it/s]\u001b[A\n",
            " 89%|████████▊ | 274/309 [00:19<00:03, 10.72it/s]\u001b[A\n",
            " 89%|████████▉ | 276/309 [00:20<00:02, 12.04it/s]\u001b[A\n",
            " 90%|████████▉ | 278/309 [00:20<00:02, 12.70it/s]\u001b[A\n",
            " 91%|█████████ | 281/309 [00:20<00:02, 12.19it/s]\u001b[A\n",
            " 92%|█████████▏| 283/309 [00:20<00:02,  8.70it/s]\u001b[A\n",
            " 93%|█████████▎| 287/309 [00:21<00:01, 11.29it/s]\u001b[A\n",
            " 94%|█████████▍| 290/309 [00:21<00:01, 13.25it/s]\u001b[A\n",
            " 95%|█████████▍| 293/309 [00:21<00:01, 15.35it/s]\u001b[A\n",
            " 96%|█████████▌| 296/309 [00:21<00:01, 12.94it/s]\u001b[A\n",
            " 96%|█████████▋| 298/309 [00:21<00:00, 12.88it/s]\u001b[A\n",
            " 98%|█████████▊| 302/309 [00:21<00:00, 15.75it/s]\u001b[A\n",
            " 99%|█████████▊| 305/309 [00:21<00:00, 17.63it/s]\u001b[A\n",
            "100%|██████████| 309/309 [00:22<00:00, 13.90it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPFtoCqdPZxj",
        "colab_type": "text"
      },
      "source": [
        "###Reading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zwsmRW3vYPMj"
      },
      "source": [
        "We now have a bunch of descriptions for the websites in our training data. How do we map this to a meaningful feature representation? As suggested above, we use the approach of assigning each feature index a specific word from the descriptions, and the feature value for each site is just the count of that specific word in its description. This is just an automatic version of the keyword-based approach we used yesterday.\n",
        "\n",
        "How do we choose which words to include as features? We could include all of them, but this would give us a lot of features for words that don't show up often enough to be helpful. Instead, we choose the 300 most frequent words.\n",
        "\n",
        "Below, we use the CountVectorizer class from scikit-learn to do the heavy-lifting for us. We train it using just the train data (so it learns which are the 300 most frequent words in train only), and then we use to featurize both the train and val data. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2tc1a2yPeW0",
        "colab_type": "text"
      },
      "source": [
        "## Exercise \n",
        "\n",
        "Fill in the last **two lines of code** that vectorizes the val data descriptions, using the train version for reference (~4 minutes). You should save the values to *bow_val_X* and *bow_val_y*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qduz6WP1JBCO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "9f0ae814-905e-4472-f028-8606ff51f4d8"
      },
      "source": [
        "vectorizer = CountVectorizer(max_features=300)\n",
        "\n",
        "vectorizer.fit(train_descriptions)\n",
        "\n",
        "def vectorize_data_descriptions(descriptions, vectorizer):\n",
        "  X = vectorizer.transform(descriptions).todense()\n",
        "  return X\n",
        "\n",
        "print('\\nPreparing train data...')\n",
        "bow_train_X = vectorize_data_descriptions(train_descriptions, vectorizer)\n",
        "bow_train_y = [label for url, html, label in train_data]\n",
        "print('\\nPreparing val data...')\n",
        "### YOUR CODE HERE ###\n",
        "bow_val_X = vectorize_data_descriptions(val_descriptions, vectorizer)\n",
        "bow_val_y = [label for url, html, label in val_data]\n",
        "\n",
        "### END CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Preparing train data...\n",
            "\n",
            "Preparing val data...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "urfxVL-MWzHW"
      },
      "source": [
        "Now we have all we need to test our bag-of-words featurization. Below, we want to use logistic regression, as before, combined with our *train_X* produced by CountVectorizer to train our fake news classification model. We also want to evaluate using our familiar metrics.\n",
        "\n",
        "## Exercise \n",
        "\n",
        "Fill in the code below that fits the model on *bow_train_X* and *bow_train_y* and outputs train accuracy, val accuracy, val confusion matrix, and val precision, recall, and F1-Score (~10 minutes)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWg6ZLbAJNFB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "7b591ffd-2824-4029-dde2-9abff726fba6"
      },
      "source": [
        "model = LogisticRegression()\n",
        "\n",
        "### YOUR CODE HERE ###\n",
        "model.fit(bow_train_X, bow_train_y)\n",
        "bow_train_y_pred = model.predict(bow_train_X)\n",
        "print('Train accuracy', accuracy_score(bow_train_y, bow_train_y_pred))\n",
        "\n",
        "\n",
        "bow_val_y_pred = model.predict(bow_val_X)\n",
        "print('Val accuracy', accuracy_score(bow_val_y, bow_val_y_pred))\n",
        "\n",
        "print('Confusion matrix:')\n",
        "print(confusion_matrix(bow_val_y, bow_val_y_pred))\n",
        "\n",
        "prf = precision_recall_fscore_support(bow_val_y, bow_val_y_pred)\n",
        "\n",
        "print('Precision:', prf[0][1])\n",
        "print('Recall:', prf[1][1])\n",
        "print('F-Score:', prf[2][1])\n",
        "### END CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train accuracy 0.8746253746253746\n",
            "Val accuracy 0.6634304207119741\n",
            "Confusion matrix:\n",
            "[[ 77  91]\n",
            " [ 13 128]]\n",
            "Precision: 0.5844748858447488\n",
            "Recall: 0.9078014184397163\n",
            "F-Score: 0.7111111111111111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lXxFvJTbXT7S"
      },
      "source": [
        "Solid! You should be getting results roughly similar to what you got yesterday, which may be surprising since we are restricting ourselves to look only at the description of a website. The strength of our approach today is that the featurization is automatic–we didn't put any work into determining the features.\n",
        "\n",
        "You might be wondering how well a model that combines both our bag-of-words approach, our keywords approach, and our domain name extension approach does. We will implement combining our featurization approaches tomorrow! For now, let's explore another approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "e8VLitfjkKZ6"
      },
      "source": [
        "## Instructor-Led Discussion: Modeling the Meaning of Websites using Word Vectors\n",
        "\n",
        "\n",
        "A shortcoming of our bag-of-words approach is that it only looks at the counts of words in the description for each website. What if we had some way of understanding the meaning of words in the description for each website?\n",
        "\n",
        "The idea of computationally extracting meaning from words is central to word vectors, which have become a cornerstone of modern deep learning on text. Word vectors are a mapping from words to vectors such that words that have similar meaning have similar word vectors. \n",
        "\n",
        "For example, the words \"good\" and \"great\" have similar word vectors, and the words \"good\" and \"planet\" have different word vectors. Thus, word vectors provide us a way to account for the meanings of words with our machine learning models.\n",
        "\n",
        "Run the below cell to load our word vectors, which come from a model called \"GloVe\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lIww33eRkucs",
        "colab": {}
      },
      "source": [
        "VEC_SIZE = 300\n",
        "glove = GloVe(name='6B', dim=VEC_SIZE)\n",
        "\n",
        "# Returns word vector for word if it exists, else return None.\n",
        "def get_word_vector(word):\n",
        "    try:\n",
        "      return glove.vectors[glove.stoi[word.lower()]].numpy()\n",
        "    except KeyError:\n",
        "      return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "05fcNrWgZcxC"
      },
      "source": [
        "We've included a handy helper function which retrieves the word vector for a word. \n",
        "\n",
        "## Exercise \n",
        "\n",
        "Let's retrieve the word vector for \"good\" using the above get_word_vector function (~30 seconds)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPk4TeQmJZ0s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "8a087249-0c70-41e8-e6c3-38eba2439e13"
      },
      "source": [
        "### YOUR CODE HERE ###\n",
        "good_vector = get_word_vector(\"good\")\n",
        "### END CODE HERE ###\n",
        "\n",
        "print('Shape of good vector:', good_vector.shape)\n",
        "print(good_vector)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of good vector: (300,)\n",
            "[-1.3602e-01 -1.1594e-01 -1.7078e-02 -2.9256e-01  1.6149e-02  8.6472e-02\n",
            "  1.5759e-03  3.4395e-01  2.1661e-01 -2.1366e+00  3.5278e-01 -2.3909e-01\n",
            " -2.2174e-01  3.6413e-01 -4.5021e-01  1.2104e-01 -1.5596e-01 -3.8906e-02\n",
            " -2.9419e-03  1.6009e-02 -1.1620e-01  3.8680e-01  3.5109e-01  9.7426e-02\n",
            " -1.2425e-02 -1.7864e-01 -2.3259e-01 -2.6960e-01  4.1083e-02 -7.6194e-02\n",
            " -2.3362e-01  2.0919e-01 -2.7264e-01  5.4967e-02 -1.8055e+00  5.6348e-01\n",
            " -1.2778e-01  2.3147e-01 -5.8820e-03 -2.6630e-01  4.1187e-01 -3.7162e-01\n",
            " -2.0600e-01 -1.9619e-01 -4.3945e-03  1.2513e-01  4.6638e-01  4.5159e-01\n",
            " -1.5000e-01  5.9589e-03  5.9070e-02 -4.1440e-01  6.1035e-02 -2.1117e-01\n",
            " -4.0988e-01  5.6393e-01  2.3021e-01  2.7240e-01  4.9364e-02  1.4239e-01\n",
            "  4.1841e-01 -1.3983e-01  3.4826e-01 -1.0745e-01 -2.5002e-01 -3.2554e-01\n",
            "  3.3343e-01 -3.5617e-01  2.0442e-01  1.4439e-01 -1.2686e-01 -7.7273e-02\n",
            " -1.9667e-01  1.0759e-01 -1.1860e-01 -2.5083e-01  1.4205e-02  2.7251e-01\n",
            " -2.3707e-01 -2.3545e-01 -1.5887e-01  1.3151e-01  6.9564e-01  2.2766e-01\n",
            "  1.8526e-01  1.5743e-01 -1.5018e-01 -1.8177e-01 -3.3527e-02 -3.3092e-01\n",
            " -2.5205e-01  5.0913e-01 -2.5607e-01 -5.3686e-01  1.3397e-01  6.7046e-02\n",
            " -9.4473e-02 -2.2270e-01 -3.1469e-01  8.5932e-02 -4.3032e-02 -2.5821e-01\n",
            " -9.5062e-02 -1.8497e-01  5.8890e-02  1.8972e-01 -1.7366e-01  2.5263e-01\n",
            " -5.4361e-01 -3.7248e-01 -4.6661e-02 -4.1657e-01 -1.7549e-03 -4.8404e-01\n",
            "  4.2090e-01 -1.2749e-03  9.4697e-03 -1.3380e-01  7.2351e-02 -1.2096e-01\n",
            " -7.2870e-02 -1.8333e-01  3.9652e-01  1.1329e-01 -6.3029e-02 -1.9702e-03\n",
            "  4.2848e-01  3.1790e-01 -1.5079e-01  2.0405e-01  2.1828e-01  2.6067e-02\n",
            "  4.3621e-02  3.9224e-03 -2.6629e-01 -2.8312e-01  5.0497e-02 -1.8993e-01\n",
            "  1.8996e-01  2.9517e-01 -1.1566e-01  4.0967e-01  2.2221e-01 -3.9778e-01\n",
            " -3.3177e-01 -1.3884e-01 -1.6829e-01 -2.0355e-01 -2.7687e-01 -1.1087e-01\n",
            " -6.7466e-01 -1.8108e-01  1.8512e-01 -9.4616e-02  1.7856e-01 -6.6997e-02\n",
            "  1.1379e-01 -9.3380e-02  5.6860e-01 -1.3365e-01  3.4636e-01 -4.1953e-01\n",
            "  1.7547e-01 -2.4277e-02 -1.2441e-01  9.2129e-02 -1.6702e-01 -1.4285e-01\n",
            "  3.1646e-01  3.0337e-01  1.4840e-01 -6.7837e-03 -1.0509e+00  2.2329e-01\n",
            "  7.5211e-02  4.4379e-02 -8.5929e-02 -1.1806e-01 -1.6632e-01 -7.8650e-02\n",
            "  2.6374e-01 -2.2052e-01  4.5582e-01 -1.5291e-01  6.2617e-02 -1.5588e-01\n",
            "  8.2398e-02 -6.8462e-02 -2.4569e-01  2.3439e-01 -3.8633e-01  2.4835e-01\n",
            "  2.5334e-01 -2.1189e-01  4.1494e-03 -4.3762e-01 -1.3426e-01 -2.4583e-01\n",
            "  1.4213e-01 -3.3973e-01  1.4643e+00  1.6414e-01  2.2135e-01  7.4099e-03\n",
            " -5.5141e-02 -2.7403e-02  3.2928e-02  1.4289e-01 -1.0049e-01 -2.2066e-01\n",
            " -3.0380e-01  6.0624e-02 -1.2408e-01 -5.4114e-01  2.4374e-01  8.0903e-02\n",
            " -7.8264e-02  8.0091e-02  9.8551e-03 -2.3077e-01  1.6006e-01  6.4075e-02\n",
            " -4.1613e-01  2.0494e-01 -1.8681e-01  3.5367e-02  2.1759e-01 -8.7823e-02\n",
            "  3.5452e-01  1.9578e-01 -1.5127e-01 -1.0545e-01  3.5650e-01 -3.8677e-01\n",
            " -6.3172e-02  3.1534e-01 -1.5887e-01 -3.1267e-01 -1.7893e-01  4.1952e-01\n",
            "  2.3261e-01  2.0943e-01  2.7013e-02  1.7388e-02 -5.9857e-01 -1.9622e-01\n",
            " -2.3672e-01  3.0032e-01  4.6926e-02 -8.5768e-02  3.6539e-01 -5.2476e-01\n",
            " -1.3618e-01  1.0868e-01  4.6307e-01  3.8502e-01  7.6317e-04 -3.8196e-01\n",
            "  7.9772e-02 -4.1744e-02  4.7625e-02 -4.1018e-02  1.7601e-01  2.4893e-01\n",
            " -1.0753e-01  3.1935e-01 -1.2762e-01 -3.5059e-01  3.5689e-04  9.3515e-03\n",
            " -8.8616e-02 -3.2785e-01  9.2063e-02 -6.1405e-02  2.9053e-01  2.2404e-02\n",
            " -1.6879e+00  2.6712e-01  3.3419e-01 -5.2533e-02 -1.9741e-01  1.3709e-01\n",
            " -5.4288e-02  5.6423e-01  1.9384e-01  1.7229e-01  2.9025e-01 -1.6124e-01\n",
            "  5.9489e-02 -3.1884e-01 -2.8343e-01  6.4321e-02 -4.1589e-01 -7.0528e-02\n",
            "  1.2410e-02 -4.0208e-01 -2.4963e-01 -3.3760e-01  7.0098e-02  2.4642e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVIt9A5bO-S0",
        "colab_type": "text"
      },
      "source": [
        "### Reading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TXL5UubKaeDB"
      },
      "source": [
        "Not too much to see here–each word vector is a vector of 300 numbers, and it's hard to interpret them from looking at the numbers. Remember that the important property of word vectors is that words with similar meaning have similar word vectors. The magic happens when we compare word vectors.\n",
        "\n",
        "Below, we have set up a demo where we compare the word vectors for two words using a comparison metric known as cosine similarity. Intuitively, cosine similarity measures the extent to which two vectors point in the same direction. You might be familiar with the fact that the cosine similarity between two vectors is the same as the cosine of the angle between the two vectors–ranging between -1 and 1. -1 means that two vectors are facing opposite directions, 0 means that they are perpindicular, and 1 means that they are facing the same direction. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBA15HjvN4I-",
        "colab_type": "text"
      },
      "source": [
        "## Instructor-Led Discussion: Comparing Word Similarities\n",
        "\n",
        "Try running the below to compare the vectors for \"good\" and \"great\", and then try other words, like \"planet\". **What do you notice that's expected and unexpected? Play around for a couple of minutes then discuss as a class.**\n",
        "\n",
        "Note that the demo runs automatically when you change either *word1* or *word2*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hZz6rcIncU7l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "0262e3af-01aa-437f-a1e5-70b9cb8a6925"
      },
      "source": [
        "#@title Word Similarity { run: \"auto\", display-mode: \"both\" }\n",
        "\n",
        "def cosine_similarity(vec1, vec2):    \n",
        "  return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
        "\n",
        "word1 = \"frog\" #@param {type:\"string\"}\n",
        "word2 = \"frogs\" #@param {type:\"string\"}\n",
        "\n",
        "print('Word 1:', word1)\n",
        "print('Word 2:', word2)\n",
        "\n",
        "def cosine_similarity_of_words(word1, word2):\n",
        "  vec1 = get_word_vector(word1)\n",
        "  vec2 = get_word_vector(word2)\n",
        "  \n",
        "  if vec1 is None:\n",
        "    print(word1, 'is not a valid word. Try another.')\n",
        "  if vec2 is None:\n",
        "    print(word2, 'is not a valid word. Try another.')\n",
        "  if vec1 is None or vec2 is None:\n",
        "    return None\n",
        "  \n",
        "  return cosine_similarity(vec1, vec2)\n",
        "  \n",
        "\n",
        "print('\\nCosine similarity:', cosine_similarity_of_words(word1, word2))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word 1: frog\n",
            "Word 2: frogs\n",
            "\n",
            "Cosine similarity: 0.6233975\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb6vzQX0O18k",
        "colab_type": "text"
      },
      "source": [
        "### Reading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GKlczvl58vTy"
      },
      "source": [
        "We can see that word embeddings appear to capture the meaning of different words–when two words are similar, the cosine similarity score is higher, and when two words are dissimilar, the cosine similarity score is lower.\n",
        "\n",
        "Word vectors are created by going over a large body of text (the vectors you are using were trained on Wikipedia in part) and noticing which words tend to occur near each-other. If word A tends to co-occur with similar words as word B, then the word vectors for words A and B are mathematically constrained to be similar. If you want to learn more about an algorithm for training word vectors, see this [helpful introduction to word2vec](https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa).\n",
        "\n",
        "Given word vectors that represent the meaning of words, what can we do with this? We can add word vectors to our feature vector, but which do we choose? It turns out that a solid approach is just to average the word vectors for all the words in the description. Averaging word vectors produces a natural way to produce vectors for sentences and other collections of words, and this is the approach we will use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l21_Y_V3Aq_F"
      },
      "source": [
        "## Exercise \n",
        "\n",
        "We want to write a function that takes a list of descriptions and turns it into an array containing the average GloVe vector for each description. **Understand the code below and then increment found_words and add vec to X[i].**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbM1bDocJwTy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def glove_transform_data_descriptions(descriptions):\n",
        "    X = np.zeros((len(descriptions), VEC_SIZE))\n",
        "    for i, description in enumerate(descriptions):\n",
        "        found_words = 0.0\n",
        "        description = description.strip()\n",
        "        for word in description.split(): \n",
        "            vec = get_word_vector(word)\n",
        "            if vec is not None:\n",
        "                ### YOUR CODE HERE ###\n",
        "                # Increment found_words and add vec to X[i].\n",
        "                found_words += 1\n",
        "                X[i] += vec\n",
        "                \n",
        "                ### END CODE HERE ###\n",
        "        # We divide the sum by the number of words added, so we have the\n",
        "        # average word vector.\n",
        "        if found_words > 0:\n",
        "            X[i] /= found_words\n",
        "            \n",
        "    return X\n",
        "  \n",
        "glove_train_X = glove_transform_data_descriptions(train_descriptions)\n",
        "glove_train_y = [label for (url, html, label) in train_data]\n",
        "\n",
        "glove_val_X = glove_transform_data_descriptions(val_descriptions)\n",
        "glove_val_y = [label for (url, html, label) in val_data]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GHXCrVToBQ63"
      },
      "source": [
        "## Exercise \n",
        "\n",
        "Then, we can evaluate our approach as we have in the past. As before, fill in the code for fitting and evaluation (~8 minutes)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVifTxrMJ8Zb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "0978f95f-fa84-4921-9bd5-b26f5872574c"
      },
      "source": [
        "model = LogisticRegression()\n",
        "### YOUR CODE HERE ###\n",
        "model.fit(glove_train_X, glove_train_y)\n",
        "glove_train_y_pred = model.predict(glove_train_X)\n",
        "print('Train accuracy', accuracy_score(glove_train_y, glove_train_y_pred))\n",
        "\n",
        "\n",
        "glove_val_y_pred = model.predict(glove_val_X)\n",
        "print('Val accuracy', accuracy_score(glove_val_y, glove_val_y_pred))\n",
        "\n",
        "print('Confusion matrix:')\n",
        "print(confusion_matrix(glove_val_y, glove_val_y_pred))\n",
        "\n",
        "prf = precision_recall_fscore_support(glove_val_y, glove_val_y_pred)\n",
        "\n",
        "print('Precision:', prf[0][1])\n",
        "print('Recall:', prf[1][1])\n",
        "print('F-Score:', prf[2][1])\n",
        "### END CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train accuracy 0.8656343656343657\n",
            "Val accuracy 0.7702265372168284\n",
            "Confusion matrix:\n",
            "[[116  52]\n",
            " [ 19 122]]\n",
            "Precision: 0.7011494252873564\n",
            "Recall: 0.8652482269503546\n",
            "F-Score: 0.7746031746031746\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bnIwqSqHDBwD"
      },
      "source": [
        "We can see that we again get solid results using a different approach. Each approach is encoding different information about websites, and so we would expect that combining them together would produce even better results. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zaqWxvcFuzv",
        "colab_type": "text"
      },
      "source": [
        "## Exercise \n",
        "\n",
        "Fill in *train_and_evaluate_model*, which trains and evaluates a logistic regression model given train_X, train_y, val_X, and val_y. Print train accuracy, val accuracy, confusion matrix, precision, recall, and F1-score, just as we have been doing the past few days (~7 minutes)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIDKiMPEKIO8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(train_X, train_y, val_X, val_y):\n",
        "  model = LogisticRegression(solver='liblinear')\n",
        "  model.fit(train_X, train_y)\n",
        "  \n",
        "  return model\n",
        "\n",
        "\n",
        "def train_and_evaluate_model(train_X, train_y, val_X, val_y):\n",
        "  model = train_model(train_X, train_y, val_X, val_y)\n",
        "  \n",
        "  ### YOUR CODE HERE ###\n",
        "  model.fit(train_X, train_y)\n",
        "  train_y_pred = model.predict(train_X)\n",
        "  print('Train accuracy', accuracy_score(train_y, train_y_pred))\n",
        " \n",
        "\n",
        "  val_y_pred = model.predict(val_X)\n",
        "  print('Val accuracy', accuracy_score(val_y, val_y_pred))\n",
        "\n",
        "  print('Confusion matrix:')\n",
        "  print(confusion_matrix(val_y, val_y_pred))\n",
        "\n",
        "  prf = precision_recall_fscore_support(val_y, val_y_pred)\n",
        "\n",
        "  print('Precision:', prf[0][1])\n",
        "  print('Recall:', prf[1][1])\n",
        "  print('F-Score:', prf[2][1])\n",
        "  ### END CODE HERE ###\n",
        "  \n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGynL44dGCRz",
        "colab_type": "text"
      },
      "source": [
        "### Review of story so far\n",
        "\n",
        "Our first approach to featurizing our data involved looking only at URLs, specifically domain name extensions. We discovered that this achieved about 60% accuracy, with many false negatives for fake news websites with an innocuous domain name extension like \".com\". This accuracy wasn't great, but it gave us a baseline to improve upon.\n",
        "\n",
        "We next moved on to keyword-based featurization, which combined the above domain name extension features with normalized counts (extracted from HTML) for a list of specific words. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATqwCXq2GPOV",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Exercise \n",
        "\n",
        "Given the functions for featurizing data before and our new helper function *train_and_evaluate_model*, train and evaluate a model on top of keyword (combined with domain name extension) featurization (~8 minutes)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E-lU22hKcHG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "0c0122dd-349e-4cf3-9fd2-d39680c28f1a"
      },
      "source": [
        "def prepare_data(data, featurizer):\n",
        "    X = []\n",
        "    y = []\n",
        "    for datapoint in data:\n",
        "        url, html, label = datapoint\n",
        "        # We convert all text in HTML to lowercase, so <p>Hello.</p> is mapped to\n",
        "        # <p>hello</p>. This will help us later when we extract features from \n",
        "        # the HTML, as we will be able to rely on the HTML being lowercase.\n",
        "        html = html.lower() \n",
        "        y.append(label)\n",
        "\n",
        "        features = featurizer(url, html)\n",
        "\n",
        "        # Gets the keys of the dictionary as descriptions, gets the values\n",
        "        # as the numerical features. Don't worry about exactly what zip does!\n",
        "        feature_descriptions, feature_values = zip(*features.items())\n",
        "\n",
        "        X.append(feature_values)\n",
        "\n",
        "    return X, y, feature_descriptions\n",
        "  \n",
        "# Gets the log count of a phrase/keyword in HTML (transforming the phrase/keyword\n",
        "# to lowercase).\n",
        "def get_normalized_count(html, phrase):\n",
        "    return math.log(1 + html.count(phrase.lower()))\n",
        "\n",
        "# Returns a dictionary mapping from plaintext feature descriptions to numerical\n",
        "# features for a (url, html) pair.\n",
        "def keyword_featurizer(url, html):\n",
        "    features = {}\n",
        "    \n",
        "    # Same as before.\n",
        "    features['.com domain'] = url.endswith('.com')\n",
        "    features['.org domain'] = url.endswith('.org')\n",
        "    features['.net domain'] = url.endswith('.net')\n",
        "    features['.info domain'] = url.endswith('.info')\n",
        "    features['.org domain'] = url.endswith('.org')\n",
        "    features['.biz domain'] = url.endswith('.biz')\n",
        "    features['.ru domain'] = url.endswith('.ru')\n",
        "    features['.co.uk domain'] = url.endswith('.co.uk')\n",
        "    features['.co domain'] = url.endswith('.co')\n",
        "    features['.tv domain'] = url.endswith('.tv')\n",
        "    features['.news domain'] = url.endswith('.news')\n",
        "    \n",
        "    keywords = ['trump', 'biden', 'clinton', 'sports', 'finance']\n",
        "    \n",
        "    for keyword in keywords:\n",
        "      features[keyword + ' keyword'] = get_normalized_count(html, keyword)\n",
        "    \n",
        "    return features\n",
        "\n",
        "\n",
        "### YOUR CODE HERE ###\n",
        "keyword_train_X, train_y,_= prepare_data(train_data, keyword_featurizer)\n",
        "keyword_val_X, val_y,_ = prepare_data(val_data, keyword_featurizer)\n",
        "\n",
        "train_and_evaluate_model(keyword_train_X, train_y,keyword_val_X, val_y)\n",
        "\n",
        "\n",
        "### END CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train accuracy 0.7922077922077922\n",
            "Val accuracy 0.7346278317152104\n",
            "Confusion matrix:\n",
            "[[106  62]\n",
            " [ 20 121]]\n",
            "Precision: 0.6612021857923497\n",
            "Recall: 0.8581560283687943\n",
            "F-Score: 0.7469135802469136\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2gWWDunGZsZ",
        "colab_type": "text"
      },
      "source": [
        "We can see that we are achieving significantly better accuracy and better balance between false negatives and false positives. As expected, it looks like actually making use of the content of the HTML is useful.\n",
        "\n",
        "Another way to take advantage of the HTML is to extract a bag-of-words (BOW) featurization from the meta description stored in the HTML. \n",
        "\n",
        "## Instructor-Led Discussion: Bag of Words featurization\n",
        "\n",
        "## Exercise \n",
        "\n",
        "As before, train and evaluate this approach with our helper function *train_and_evaluate_model* (~5 minutes)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2G_QtzvKqrc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "2a723b7a-fa57-4d60-cec5-2f68bc4e34c3"
      },
      "source": [
        "vectorizer = CountVectorizer(max_features=300)\n",
        "\n",
        "vectorizer.fit(train_descriptions)\n",
        "\n",
        "def vectorize_data_descriptions(data_descriptions, vectorizer):\n",
        "  X = vectorizer.transform(data_descriptions).todense()\n",
        "  return X\n",
        "\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "# Note that you can use train_y and val_y from before, since these are the\n",
        "# same for both the keyword approach and the BOW approach.\n",
        "\n",
        "bow_train_X = vectorize_data_descriptions(train_descriptions, vectorizer)\n",
        "bow_val_X = vectorize_data_descriptions(val_descriptions, vectorizer)\n",
        "\n",
        "train_and_evaluate_model(bow_train_X, train_y,bow_val_X, val_y)\n",
        "\n",
        "\n",
        "\n",
        "### END CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train accuracy 0.8746253746253746\n",
            "Val accuracy 0.6634304207119741\n",
            "Confusion matrix:\n",
            "[[ 77  91]\n",
            " [ 13 128]]\n",
            "Precision: 0.5844748858447488\n",
            "Recall: 0.9078014184397163\n",
            "F-Score: 0.7111111111111111\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mAbl_A0GjvG",
        "colab_type": "text"
      },
      "source": [
        "We can see that we are getting similar results, without necessarily using the same information as the keywords-based approach. We then asked whether we could do better by making use of word vectors, which encode the meaning of different words. We found that averaging the word vectors for words in the meta description was a useful approach. \n",
        "\n",
        "## Exercise \n",
        "\n",
        "As before, review the approach and complete the code for training and evaluation (~5 minutes)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSXDmQGlLKlL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "84887f0a-f224-483b-b71e-2e60c4988de8"
      },
      "source": [
        "VEC_SIZE = 300\n",
        "glove = GloVe(name='6B', dim=VEC_SIZE)\n",
        "\n",
        "# Returns word vector for word if it exists, else return None.\n",
        "def get_word_vector(word):\n",
        "    try:\n",
        "      return glove.vectors[glove.stoi[word.lower()]].numpy()\n",
        "    except KeyError:\n",
        "      return None\n",
        "\n",
        "def glove_transform_data_descriptions(descriptions):\n",
        "    X = np.zeros((len(descriptions), VEC_SIZE))\n",
        "    for i, description in enumerate(descriptions):\n",
        "        found_words = 0.0\n",
        "        description = description.strip()\n",
        "        for word in description.split(): \n",
        "            vec = get_word_vector(word)\n",
        "            if vec is not None:\n",
        "                ### YOUR CODE HERE ###\n",
        "                # Increment found_words and add vec to X[i].\n",
        "                found_words += 1\n",
        "                X[i] += vec\n",
        "                ### END CODE HERE ###\n",
        "        # We divide the sum by the number of words added, so we have the\n",
        "        # average word vector.\n",
        "        if found_words > 0:\n",
        "            X[i] /= found_words\n",
        "            \n",
        "    return X\n",
        "  \n",
        "  \n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "# Note that you can use train_y and val_y from before, since these are the\n",
        "# same for both the keyword approach and the BOW approach.\n",
        "glove_train_X = glove_transform_data_descriptions(train_descriptions)\n",
        "glove_val_X = glove_transform_data_descriptions(val_descriptions)\n",
        "\n",
        "train_and_evaluate_model(glove_train_X, train_y,glove_val_X, val_y)\n",
        "\n",
        "\n",
        "### END CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train accuracy 0.8656343656343657\n",
            "Val accuracy 0.7702265372168284\n",
            "Confusion matrix:\n",
            "[[116  52]\n",
            " [ 19 122]]\n",
            "Precision: 0.7011494252873564\n",
            "Recall: 0.8652482269503546\n",
            "F-Score: 0.7746031746031746\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsZiH04uGtSC",
        "colab_type": "text"
      },
      "source": [
        "Again, solid results using a completely different approach.\n",
        "\n",
        "### Instructor-Led Discussion: Combining Approaches\n",
        "\n",
        "A natural question to ask now is whether we can combine the above featurization approaches for improved results. It turns out we can, by concatenating the feature vectors for each website produced using each of the three above approaches. Below we provide a handy helper function that takes in a list of multiple train_X produced using different featurization approaches (e.g., [keyword_train_X, bow_train_X, glove_train_X]) and combines them into *combined_train_X*. It can do this for val_X as well.\n",
        "\n",
        "As an example, if our keyword-based approach has 15 features, our BOW approach has 300, and our GloVe approach has 300, then our combined approach has 615 features.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIyk8GsmHOtC",
        "colab_type": "text"
      },
      "source": [
        "## Exercise \n",
        "\n",
        "Complete the below code for training and evaluating the combined approach (~8 minutes)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MD1gl146LW2E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "3320ca10-c73d-426b-b0b1-4c2dd972cb66"
      },
      "source": [
        "def combine_features(X_list):\n",
        "  return np.concatenate(X_list, axis=1)\n",
        "\n",
        "### YOUR CODE HERE ###\n",
        "# First, produce combined_train_X and combined_val_X using 2 calls to \n",
        "# combine_features, using keyword_train_X, bow_train_X, glove_train_X\n",
        "# and keyword_val_X, bow_val_X, glove_val_X from before.\n",
        "combined_train_X = combine_features([keyword_train_X, bow_train_X, glove_train_X])\n",
        "combined_val_X = combine_features([keyword_val_X, bow_val_X, glove_val_X]) \n",
        "\n",
        "train_and_evaluate_model(combined_train_X, train_y,combined_val_X, val_y)\n",
        "\n",
        "\n",
        "### END CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train accuracy 0.9175824175824175\n",
            "Val accuracy 0.7993527508090615\n",
            "Confusion matrix:\n",
            "[[119  49]\n",
            " [ 13 128]]\n",
            "Precision: 0.7231638418079096\n",
            "Recall: 0.9078014184397163\n",
            "F-Score: 0.8050314465408805\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZlqslCAIISn",
        "colab_type": "text"
      },
      "source": [
        "## Exercise \n",
        "\n",
        "Now, make changes to the keyword, BOW, and GloVe featurization code above to improve performance. For example, add keywords to the keyword featurization code, play around with different values for *max_features* and other parameters for [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html), and try summing instead of averaging word vectors (~15 minutes).\n",
        "\n",
        "## Exercise \n",
        "\n",
        "Once you feel satisfied with your results, play around with the below demo, which scrapes a new website live and runs your trained classification model on it! The below code assumes you have run the code above and have not changed the names of any of the featurization functions. It combines the three featurization approaches. Go through the code below and make sure you understand what it is doing! Feel free to also make changes (e.g. if you only want to use two featurization approaches) (~15 minutes). Note that output is shown below the *curr_url* field."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0pttvKMIPfq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bd6fe411-2b4f-4386-fdd4-d09ebd210834"
      },
      "source": [
        "#@title Live Fake News Classification Demo { run: \"auto\", vertical-output: true, display-mode: \"both\" }\n",
        "def get_data_pair(url):\n",
        "  if not url.startswith('http'):\n",
        "      url = 'http://' + url\n",
        "  url_pretty = url\n",
        "  if url_pretty.startswith('http://'):\n",
        "      url_pretty = url_pretty[7:]\n",
        "  if url_pretty.startswith('https://'):\n",
        "      url_pretty = url_pretty[8:]\n",
        "      \n",
        "  # Scrape website for HTML\n",
        "  response = requests.get(url, timeout=10)\n",
        "  htmltext = response.text\n",
        "  \n",
        "  return url_pretty, htmltext\n",
        "\n",
        "curr_url = \"https://www.cnn.com\" #@param {type:\"string\"}\n",
        "\n",
        "url, html = get_data_pair(curr_url)\n",
        "\n",
        "# Call on the output of *keyword_featurizer* or something similar\n",
        "# to transform it into a format that allows for concatenation. See\n",
        "# example below.\n",
        "def dict_to_features(features_dict):\n",
        "  X = np.array(list(features_dict.values())).astype('float')\n",
        "  X = X[np.newaxis, :]\n",
        "  return X\n",
        "def featurize_data_pair(url, html):\n",
        "  # Approach 1.\n",
        "  keyword_X = dict_to_features(keyword_featurizer(url, html))\n",
        "  # Approach 2.\n",
        "  description = get_description_from_html(html)\n",
        "  \n",
        "  bow_X = vectorize_data_descriptions([description], vectorizer)\n",
        "  \n",
        "  # Approach 3.\n",
        "  glove_X = glove_transform_data_descriptions([description])\n",
        "  \n",
        "  X = combine_features([keyword_X, bow_X, glove_X])\n",
        "  \n",
        "  return X\n",
        "\n",
        "curr_X = featurize_data_pair(url, html)\n",
        "\n",
        "model = train_model(combined_train_X, train_y, combined_val_X, val_y)\n",
        "\n",
        "curr_y = model.predict(curr_X)[0]\n",
        "  \n",
        "  \n",
        "if curr_y < .5:\n",
        "  print(curr_url, 'appears to be real.')\n",
        "else:\n",
        "  print(curr_url, 'appears to be fake.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://www.cnn.com appears to be real.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5gVcrIYIVpR",
        "colab_type": "text"
      },
      "source": [
        "## Exercise \n",
        "\n",
        "After playing around with your model live, do you have any ideas for how to improve it? \n",
        "\n",
        "1. Write down three ideas for how to improve it. \n",
        "\n",
        "2. Then, the make changes above and see what impact they have on the final model.\n",
        "\n",
        "Once you are done, we will provide code to test your model on the test data. Note that you should only run on the test data once. When you have results on the test data, share with your instructor, as the team with the best results will be rewarded!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOmNnnGvLpZd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "133b303a-6404-4925-8c9c-a72fa40d4f58"
      },
      "source": [
        "### PUT TEST CODE HERE ###\n",
        "with open(os.path.join(basepath, 'test_data.pkl'), 'rb') as f:\n",
        "  test_data = pickle.load(f)\n",
        "  \n",
        "model = train_model(combined_train_X, train_y, combined_val_X, val_y)\n",
        "val_y_pred = model.predict(combined_val_X)\n",
        "print('Val accuracy', accuracy_score(val_y, val_y_pred))\n",
        "\n",
        "print('Loading test data...')\n",
        "test_X = []\n",
        "for url, html, label in test_data:\n",
        "  curr_X = np.array(featurize_data_pair(url, html))\n",
        "  test_X.append(curr_X[0])\n",
        "  \n",
        "test_X = np.array(test_X)\n",
        "\n",
        "test_y = [label for url, html, label in test_data]\n",
        "\n",
        "print('Done loading test data...')\n",
        "\n",
        "test_y_pred = model.predict(test_X)\n",
        "\n",
        "print('Test accuracy', accuracy_score(test_y, test_y_pred))\n",
        "\n",
        "print('Confusion matrix:')\n",
        "print(confusion_matrix(test_y, test_y_pred))\n",
        "\n",
        "prf = precision_recall_fscore_support(test_y, test_y_pred)\n",
        "\n",
        "print('Precision:', prf[0][1])\n",
        "print('Recall:', prf[1][1])\n",
        "print('F-Score:', prf[2][1])\n",
        "\n",
        "### END CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-fc1ee598692e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### PUT TEST CODE HERE ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test_data.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_train_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombined_val_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B93DXnu8Ik0S",
        "colab_type": "text"
      },
      "source": [
        "A huge congratulations on completing the project!"
      ]
    }
  ]
}